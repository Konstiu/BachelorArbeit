{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8472f57d-b108-4d04-884d-9ad9f5a90bc9",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916aeb8c-77e0-4cd2-b080-c26d3c07b779",
   "metadata": {},
   "source": [
    "### 1.1 Imports\n",
    "\n",
    "Here we pull in all standard and third-party libraries used across the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84d70150-836b-421b-a587-b14cb8218714",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import List, Dict, Any\n",
    "from collections import defaultdict\n",
    "from git import Repo, BadName\n",
    "from pathlib import Path\n",
    "import json\n",
    "import requests\n",
    "import urllib.parse\n",
    "from datetime import datetime\n",
    "from packaging.version import Version, InvalidVersion\n",
    "import pandas as pd\n",
    "import tomli\n",
    "import ast\n",
    "import fnmatch\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200b28dd",
   "metadata": {},
   "source": [
    "### 1.2 Global Variables\n",
    "\n",
    "Initialize caches, default parameters, and any constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7d5c404",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The repository to analyze\n",
    "repo_name = \"mitre/caldera\"\n",
    "\n",
    "repo_url = \"https://github.com/\" + repo_name\n",
    "destination_path = \"./\" + repo_name\n",
    "repo_path = repo_name\n",
    "prefix = repo_name.split(\"/\")[0] + \"_\" + repo_name.split(\"/\")[1]\n",
    "\n",
    "\n",
    "\n",
    "# The caches for python/js, maven, and gradle\n",
    "file_cache = {}\n",
    "properties_cache = {}\n",
    "NAMESPACE = {'mvn': 'http://maven.apache.org/POM/4.0.0'}\n",
    "properties = {}\n",
    "file_cache_python = {}\n",
    "\n",
    "\n",
    "\n",
    "# Caches for the versions\n",
    "commits_with_date = {}\n",
    "cached_versions = defaultdict(dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aacf03-c08b-4b89-b5bf-019b53de65d4",
   "metadata": {},
   "source": [
    "## 2. Function Definitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5c7ee4",
   "metadata": {},
   "source": [
    "\n",
    "### 2.1 Compute Metrics per Commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a45748c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_per_commit_file_metrics(json_path, weeks=13):\n",
    "    df = pd.read_json(json_path)\n",
    "    df['commit_date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df = df[\n",
    "        df['change_type'].isin(['added', 'updated']) &\n",
    "        df['commit_date'].notna()\n",
    "    ].rename(columns={'filename': 'file_path', 'commit': 'commit_hash'})\n",
    "\n",
    "    # <<-- Neu: Numeric Casting\n",
    "    for col in ['latency', 'proj_version_lag', 'criticality']:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    # Ende Casting -->>\n",
    "\n",
    "    # 2) Quartals-Fenster etc. wie gehabt\n",
    "    start = df['commit_date'].min().normalize()\n",
    "    end   = df['commit_date'].max().normalize() + pd.Timedelta(weeks=weeks)\n",
    "    bins  = pd.date_range(start, end, freq=f'{weeks}W')\n",
    "    df['interval_start'] = pd.cut(df['commit_date'], bins=bins, labels=bins[:-1])\n",
    "\n",
    "    # 3a) proj_update_frequency pro Intervall\n",
    "    unique_per_quarter = (\n",
    "        df\n",
    "        .groupby('interval_start', observed=False)['dependency']\n",
    "        .nunique()\n",
    "        .reset_index(name='proj_update_frequency')\n",
    "    )\n",
    "\n",
    "    # 3b) proj_mtbu pro Dependency\n",
    "    dep_intervals = (\n",
    "        df\n",
    "        .sort_values(['dependency', 'commit_date'])\n",
    "        .groupby('dependency')['commit_date']\n",
    "        .apply(lambda x: x.diff().dt.total_seconds().dropna() / (3600 * 24))\n",
    "        .reset_index(name='delta_days')\n",
    "    )\n",
    "    avg_interval = (\n",
    "        dep_intervals\n",
    "        .groupby('dependency')['delta_days']\n",
    "        .mean()\n",
    "        .reset_index(name='avg_update_interval_days')\n",
    "    )\n",
    "    avg_interval['avg_update_interval_weeks'] = avg_interval['avg_update_interval_days'] / 7\n",
    "\n",
    "    # 4) Merge zur√ºck\n",
    "    df = (\n",
    "        df\n",
    "        .merge(unique_per_quarter, on='interval_start', how='left')\n",
    "        .merge(avg_interval[['dependency','avg_update_interval_weeks']], on='dependency', how='left')\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # 5) Gruppieren und Aggregieren\n",
    "    agg = (\n",
    "        df\n",
    "        .groupby(['commit_hash','commit_date','file_path'])\n",
    "        .agg(\n",
    "            proj_update_frequency      = ('proj_update_frequency',     'first'),\n",
    "            proj_mtbu                  = ('avg_update_interval_weeks', 'mean'),\n",
    "            proj_update_latency        = ('latency',                   'mean'),\n",
    "            proj_version_lag           = ('proj_version_lag',          'mean'),\n",
    "            proj_criticality_scoreAVG  = ('criticality',               'mean'),\n",
    "            proj_criticality_scoreSUM  = ('criticality',               'sum'),\n",
    "            proj_alpha_best            = ('alpha/beta/rc',             'sum'),\n",
    "            proj_alpha_used            = ('is_prerelease_new',         'sum'),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    for col in ['proj_mtbu','proj_update_latency','proj_version_lag','proj_criticality_scoreAVG', 'proj_criticality_scoreSUM']:\n",
    "        agg[col] = agg[col].round(2)\n",
    "    \n",
    "    agg = agg.sort_values('commit_date').reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    num_cols = ['proj_update_frequency', 'proj_mtbu', 'proj_update_latency', 'proj_version_lag', 'proj_criticality_scoreAVG']\n",
    "\n",
    "    agg[num_cols] = agg[num_cols].where(agg[num_cols].notna(), 'N/A')\n",
    "    agg['proj_criticality_scoreSUM'] = agg['proj_criticality_scoreSUM'].astype(object)\n",
    "    mask = agg['proj_criticality_scoreAVG'] == 'N/A'\n",
    "    agg.loc[mask, 'proj_criticality_scoreSUM'] = 'N/A'\n",
    "\n",
    "    return agg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817b78f1-e391-435a-a0c4-d9c7ef3a9105",
   "metadata": {},
   "source": [
    "### 2.2 Process Commits (POM, Gradle, Py/JS)\n",
    "\n",
    "Three parallel routines to walk each commit and process changed files by type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "327d23d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_commits_pom(repo, commits_with_changes):\n",
    "    dependencies_over_time = {}\n",
    "    dependencies_snapshot = {}\n",
    "\n",
    "    for commit_hash, changed_files in tqdm(commits_with_changes.items(), desc=\"Processing commits\"):\n",
    "        dependencies_snapshot = process_commit_pom(repo, commit_hash, changed_files, dependencies_snapshot)\n",
    "        dependencies_over_time[commit_hash] = dependencies_snapshot.copy()\n",
    "\n",
    "    return dependencies_over_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d475a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_commits_gradle(repo, commits_with_changes):\n",
    "    dependencies_over_time = {}\n",
    "    dependencies_snapshot = {}\n",
    "    all_dependencies = []\n",
    "    properties_by_commit = {}\n",
    "\n",
    "    for commit_hash, changed_files in tqdm(commits_with_changes.items(), desc=\"Processing commits\"):\n",
    "        dependencies_snapshot, all_dependencies = process_commit_gradle(\n",
    "            repo, commit_hash, changed_files, dependencies_snapshot, all_dependencies, properties_by_commit\n",
    "        )\n",
    "        dependencies_over_time[commit_hash] = dependencies_snapshot.copy()\n",
    "\n",
    "    return dependencies_over_time, all_dependencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce374bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_commits_py_js(repo_path, commits_with_files):\n",
    "    for sha, files in tqdm(commits_with_files.items(), desc=\"Processing commits\"):\n",
    "        for file_path in files:\n",
    "            if file_path.endswith(\"requirements.txt\"):\n",
    "                process_commit_file_content(repo_path, sha, file_path, \"py\")\n",
    "            elif file_path.endswith(\"package.json\"):\n",
    "                process_commit_file_content(repo_path, sha, file_path, \"js\")\n",
    "            elif file_path.endswith(\"setup.py\"):\n",
    "                process_commit_file_content(repo_path, sha, file_path, \"py\")\n",
    "            elif file_path.endswith(\"pyproject.toml\"):\n",
    "                process_commit_file_content(repo_path, sha, file_path, \"py\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fd7f31-08dc-45ba-a672-5f24f54b0e10",
   "metadata": {},
   "source": [
    "### 2.3 Save Merged Data\n",
    "\n",
    "A small helper to dump the combined dependency snapshot to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92581deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_merged_data(merged_data: Dict, output_file: str):\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(merged_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e42a97a-a069-4ab1-a7fb-7354f551cd30",
   "metadata": {},
   "source": [
    "### 2.4 Find Files\n",
    "\n",
    "Functions to locate all POMs, Gradle files, `requirements.txt`, `package.json`, etc.:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d56039a9-fe48-4c56-9833-7e77586a1ec6",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_all_files(repo_path):\n",
    "    found_files = []\n",
    "    for root, dirs, file_list in os.walk(repo_path):  # renamed 'files' to 'file_list'\n",
    "        for file in file_list:\n",
    "            if file == \"pom.xml\" or file == \"requirements.txt\" or file == \"package.json\" or file == \"setup.py\" or file == \"pyproject.toml\":\n",
    "                full_path = os.path.join(root, file)\n",
    "                found_files.append(full_path)\n",
    "    return found_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c788b6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_gradle_files(repo_path):\n",
    "    gradle_files = []\n",
    "    patterns = [\n",
    "        \"build.gradle\",\n",
    "        \"settings.gradle\",\n",
    "        \"gradle.properties\",\n",
    "        \"build.gradle.kts\",\n",
    "        \"settings.gradle.kts\",\n",
    "        \"*.gradle\",\n",
    "        \"*.gradle.kts\"\n",
    "    ]\n",
    "\n",
    "    for root, dirs, files in os.walk(repo_path):\n",
    "        for file in files:\n",
    "            for pattern in patterns:\n",
    "                if fnmatch.fnmatch(file, pattern):\n",
    "                    full_path = os.path.join(root, file)\n",
    "                    gradle_files.append(full_path)\n",
    "                    break\n",
    "    return gradle_files"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7802a759-8d14-46fd-8112-d3bc91d79d75",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "def find_commits_with_changes(repo_path, file_paths):\n",
    "    repo = Repo(repo_path)\n",
    "    commits_with_changes = {}\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        rel_path = os.path.relpath(file_path, repo_path)\n",
    "        commits = list(repo.iter_commits(paths=rel_path))\n",
    "        \n",
    "        commits_with_changes[rel_path] = [commit.hexsha for commit in commits]\n",
    "    \n",
    "    return commits_with_changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a85e5a-a9ae-4297-b938-7db48cd3c340",
   "metadata": {},
   "source": [
    "### 2.5 Commit History Helpers\n",
    "\n",
    "Load commits and filter to those touching our target files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d8cd0a8-7fd8-420f-bfdf-cacc4fbbfdb3",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_all_relevant_commits(repo_path: str, file_paths: List[str]) -> Dict[str, List[str]]:\n",
    "    repo = Repo(repo_path)\n",
    "    commits_with_changes = {}\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        rel_path = os.path.relpath(file_path, repo_path)\n",
    "\n",
    "        # Get all commits that modified the file\n",
    "        result = subprocess.run(\n",
    "            [\"git\", \"-C\", repo_path, \"log\", \"--follow\", \"--pretty=format:%H\", \"--name-only\", \"--\", rel_path],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "\n",
    "        lines = result.stdout.strip().split('\\n')\n",
    "        current_commit = None\n",
    "\n",
    "        for line in lines:\n",
    "            stripped_line = line.strip()\n",
    "            \n",
    "            if len(stripped_line) == 40 and all(c in '0123456789abcdef' for c in stripped_line):  # Check if it looks like a SHA-1 hash\n",
    "                current_commit = stripped_line\n",
    "                if current_commit not in commits_with_changes:\n",
    "                    commits_with_changes[current_commit] = set()  # Using set to avoid duplicates\n",
    "            elif current_commit:\n",
    "                modified_file = stripped_line\n",
    "                if modified_file:  # Ensure we don't add empty lines\n",
    "                    commits_with_changes[current_commit].add(modified_file)\n",
    "\n",
    "    # Convert commits to a sorted list based on commit date\n",
    "    commit_objects = []\n",
    "    for commit_hash in commits_with_changes.keys():\n",
    "        try:\n",
    "            commit_obj = repo.commit(commit_hash)\n",
    "            commit_objects.append((commit_obj, commit_hash))\n",
    "        except BadName:\n",
    "            print(f\"Skipping invalid commit hash: {commit_hash}\")  # Warn if a bad commit hash is found\n",
    "\n",
    "    sorted_commits = sorted(commit_objects, key=lambda x: x[0].committed_date)\n",
    "    \n",
    "    # Creating a sorted dictionary of commits with their modified files\n",
    "    sorted_commits_with_changes = {\n",
    "        commit_hash: list(commits_with_changes[commit_hash]) for _, commit_hash in sorted_commits\n",
    "        }\n",
    "    \n",
    "    global commits_with_date\n",
    "    commits_with_date = {\n",
    "        commit_hash: repo.commit(commit_hash).committed_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        for _, commit_hash in sorted_commits\n",
    "    }\n",
    "\n",
    "    return sorted_commits_with_changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d964731b",
   "metadata": {},
   "source": [
    "### 2.6 Parse POM XML\n",
    "\n",
    "Extract `<dependency>` entries (groupId\\:artifactId\\:version):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a2cf214",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_pom_xml(content: str, file_path: str, properties: Dict[str, str] = None) -> Dict[str, str]:\n",
    "    if properties is None:\n",
    "        properties = {}\n",
    "    else:\n",
    "        properties = properties.copy()  # Avoid side effects\n",
    "\n",
    "    dependencies = {}\n",
    "\n",
    "    try:\n",
    "        root = ET.fromstring(content)\n",
    "\n",
    "        # Update namespace if needed\n",
    "        if 'xmlns' in root.attrib:\n",
    "            NAMESPACE['mvn'] = root.attrib['xmlns']\n",
    "\n",
    "        # Load project properties\n",
    "        for prop in root.findall(\".//mvn:properties/*\", NAMESPACE):\n",
    "            if prop.tag and prop.text:\n",
    "                prop_name = prop.tag.split('}')[-1]\n",
    "                properties[prop_name] = prop.text.strip()\n",
    "                properties_cache[prop_name] = properties[prop_name]\n",
    "        \n",
    "        # Load project and parent versions as fallback properties\n",
    "        project_version = root.find(\".//mvn:version\", NAMESPACE)\n",
    "        if project_version is not None and project_version.text:\n",
    "            properties[\"project.version\"] = project_version.text.strip()\n",
    "\n",
    "        parent_version = root.find(\".//mvn:parent/mvn:version\", NAMESPACE)\n",
    "        if parent_version is not None and parent_version.text:\n",
    "            properties[\"parent.version\"] = parent_version.text.strip()\n",
    "\n",
    "        parent_info = {}\n",
    "        relative_path_elem = root.find(\".//mvn:parent/mvn:relativePath\", NAMESPACE)\n",
    "        if relative_path_elem is not None and relative_path_elem.text:\n",
    "            parent_info = {\"parent_pom_path\": relative_path_elem.text.strip()}\n",
    "\n",
    "        # Read dependencies\n",
    "        for dependency in root.findall(\".//mvn:dependency\", NAMESPACE):\n",
    "            group_id = dependency.find(\"mvn:groupId\", NAMESPACE)\n",
    "            artifact_id = dependency.find(\"mvn:artifactId\", NAMESPACE)\n",
    "            version = dependency.find(\"mvn:version\", NAMESPACE)\n",
    "\n",
    "            if group_id is not None and artifact_id is not None:\n",
    "                dep_key = f\"{group_id.text.strip()}:{artifact_id.text.strip()}\"\n",
    "                if \"${\" in dep_key and \"}\" in dep_key:\n",
    "                    dep_key = (resolve_cached(dep_key))\n",
    "\n",
    "                if version is not None and version.text:\n",
    "                    version_text = version.text.strip()\n",
    "                    if version_text.startswith(\"${\") and version_text.endswith(\"}\"):\n",
    "                        prop_name = version_text[2:-1]\n",
    "                        resolved_version = properties.get(prop_name, \"UNRESOLVED\")\n",
    "                        if resolved_version == \"UNRESOLVED\":\n",
    "                            resolved_version = resolve_from_parent(prop_name, file_path, parent_info)\n",
    "                            if (resolved_version ==\"UNRESOLVED\"):\n",
    "                                resolved_version = properties_cache.get(prop_name, \"UNRESOLVED\")\n",
    "                        if resolved_version.startswith(\"${\") and resolved_version.endswith(\"}\"):\n",
    "                            resolved_version = resolve_cached(resolved_version)\n",
    "                        dependencies[dep_key] = resolved_version\n",
    "                    else:\n",
    "                        dependencies[dep_key] = version_text\n",
    "                #else:\n",
    "                    #dependencies[dep_key] = \"UNSPECIFIED\"\n",
    "\n",
    "    except ET.ParseError as e:\n",
    "        print(f\"XML parsing error: {e}\")\n",
    "\n",
    "    return dependencies\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cbabf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def resolve_cached(prop_name, visited=None):\n",
    "    matches = re.findall(r\"\\$\\{([^}]+)\\}\", prop_name)\n",
    "    if not matches:\n",
    "        return properties_cache.get(prop_name)  # nothing to resolve, return as is\n",
    "\n",
    "    resolved = prop_name\n",
    "    for match in matches:\n",
    "        inner_value = resolve_cached(match)\n",
    "        if inner_value is None:\n",
    "            return match\n",
    "        resolved = resolved.replace(f\"${{{match}}}\", inner_value)\n",
    "\n",
    "    return resolved\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c47fbc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def resolve_from_parent(prop_name: str, file_path: str, properties: Dict[str, str]) -> str:\n",
    "    parent_pom_path = properties.get(\"parent_pom_path\")   \n",
    "    if parent_pom_path is None:\n",
    "        final_path = \"pom.xml\"\n",
    "        parent_file = str(final_path).replace(\"\\\\\", \"/\")\n",
    "        if parent_file not in file_cache:\n",
    "            return \"UNRESOLVED\"\n",
    "        content = file_cache[parent_file]\n",
    "        parent_prop_value = get_all_properties(content, prop_name, final_path, {})\n",
    "        return parent_prop_value\n",
    "\n",
    "    file_path = Path(file_path) if not isinstance(file_path, Path) else file_path\n",
    "    parent_pom_path = Path(parent_pom_path) if not isinstance(parent_pom_path, Path) else parent_pom_path\n",
    "    combined = file_path.parent / parent_pom_path\n",
    "\n",
    "    stack = []\n",
    "    for part in combined.parts:\n",
    "        if part == \"..\":\n",
    "            if stack and stack[-1] != \"..\":\n",
    "                stack.pop()\n",
    "            else:\n",
    "                stack.append(part)\n",
    "        elif part != \".\":\n",
    "            stack.append(part)\n",
    "    final_path = Path(*stack)\n",
    "    parent_file = str(final_path).replace(\"\\\\\", \"/\")\n",
    "    if parent_file not in file_cache:\n",
    "        #print(f\"[DEBUG] Skipping missing file: {parent_file}\")\n",
    "        return \"UNRESOLVED\"\n",
    "    content = file_cache[parent_file]\n",
    "    parent_prop_value = get_all_properties(content, prop_name, final_path, {})\n",
    "    return parent_prop_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "580169aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_all_properties(content: str, target_prop: str, file_path: str, properties: Dict[str, str] = None):\n",
    "    if properties is None:\n",
    "        properties = {}\n",
    "    else:\n",
    "        properties = properties.copy()  # Avoid side effects\n",
    "\n",
    "    try:\n",
    "        root = ET.fromstring(content)\n",
    "\n",
    "        if 'xmlns' in root.attrib:\n",
    "            NAMESPACE['mvn'] = root.attrib['xmlns']\n",
    "\n",
    "        # Properly assign keys without shadowing target_prop\n",
    "        for prop in root.findall(\".//mvn:properties/*\", NAMESPACE):\n",
    "            if prop.tag and prop.text:\n",
    "                key = prop.tag.split('}')[-1]\n",
    "                properties[key] = prop.text.strip()\n",
    "\n",
    "        return properties.get(target_prop, \"UNRESOLVED\")\n",
    "    except Exception as e:\n",
    "        return \"UNRESOLVED\"    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47172240",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_commit_pom(repo, commit_hash, changed_files, dependencies_snapshot):\n",
    "    for file_path in changed_files:\n",
    "        if file_path.endswith(\"pom.xml\"):\n",
    "            content = load_file_at_commit_pom(repo, commit_hash, file_path)\n",
    "            if content:\n",
    "                new_dependencies = parse_pom_xml(content, file_path, properties)\n",
    "                dependencies_snapshot[file_path] = new_dependencies\n",
    "    return dependencies_snapshot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2496a3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: System‚Äêname normalization\n",
    "def normalize_system(system: str) -> str:\n",
    "    \"\"\"\n",
    "    Map generic system names to the deps.dev API identifiers.\n",
    "    \"\"\"\n",
    "    mapping = {\n",
    "        \"java\": \"maven\",\n",
    "        \"javascript\": \"npm\",\n",
    "        \"python\": \"pypi\",\n",
    "        \"go\": \"go\"\n",
    "    }\n",
    "    return mapping.get(system, system)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e374e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Version parsing helper\n",
    "def get_major_version(version_str: str):\n",
    "    \"\"\"\n",
    "    Safely extract the major version number, or return None if invalid.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return Version(version_str).major\n",
    "    except InvalidVersion:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4983e432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Fetching & caching package data\n",
    "def fetch_package_data(package_name: str, system: str) -> dict:\n",
    "    \"\"\"\n",
    "    Hit the deps.dev API and return the raw JSON for a given package.\n",
    "    \"\"\"\n",
    "    encoded = urllib.parse.quote(package_name, safe=\"\")\n",
    "    url = f\"https://api.deps.dev/v3alpha/systems/{system}/packages/{encoded}\"\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d27cd9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_cached_package_data(package_name: str, system: str) -> dict:\n",
    "    \"\"\"\n",
    "    Return cached data if available, otherwise fetch and cache.\n",
    "    \"\"\"\n",
    "    '''if package_name not in cached_versions[system]:\n",
    "        try:\n",
    "            cached_versions[system][package_name] = fetch_package_data(package_name, system)\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"‚ùå Failed to fetch data for {package_name} ({system}): {e}\")\n",
    "            cached_versions[system][package_name] = {}\n",
    "    return cached_versions[system][package_name]\n",
    "    '''\n",
    "    LogFile = open(\"LOG_FILE.txt\", \"a\")\n",
    "    if package_name not in cached_versions[system]:\n",
    "        try:\n",
    "            # 1) root data\n",
    "            data = fetch_package_data(package_name, system)\n",
    "            try:\n",
    "                # 2) pick a version (first in the list)\n",
    "                versions = data.get(\"versions\", [])\n",
    "                if not versions:\n",
    "                    raise RuntimeError(f\"No versions found for {package_name}\")\n",
    "                # if versions is a list of strings:\n",
    "                chosen_version = versions[0].get(\"versionKey\", {}).get(\"version\")\n",
    "                # if it's list of objects, adjust to versions[0][\"version\"]\n",
    "\n",
    "                # 3) fetch version-specific metadata\n",
    "                pkg_enc = urllib.parse.quote(package_name, safe=\"\")\n",
    "                ver_url = (\n",
    "                    f\"https://api.deps.dev/v3/systems/{system}\"\n",
    "                    f\"/packages/{pkg_enc}/versions/{chosen_version}\"\n",
    "                )\n",
    "                ver_resp = requests.get(ver_url)\n",
    "                ver_resp.raise_for_status()\n",
    "                version_data = ver_resp.json()\n",
    "                #print(version_data)\n",
    "                #data[\"version_data\"] = version_data\n",
    "\n",
    "                # 4) fetch OSSF score if available\n",
    "                rel = version_data.get(\"relatedProjects\", [])\n",
    "                if rel:\n",
    "                    project_key = rel[0].get(\"projectKey\", {}).get(\"id\")\n",
    "                else:\n",
    "                    project_key = None\n",
    "\n",
    "                if project_key and project_key.lower() != \"null\":\n",
    "                    pk_enc = urllib.parse.quote(project_key, safe=\"\")\n",
    "                    ossf_url = f\"https://api.deps.dev/v3/projects/{pk_enc}\"\n",
    "                    ossf_resp = requests.get(ossf_url)\n",
    "                    if ossf_resp.ok:\n",
    "                        #print(f\"OSSF score for {package_name} (npm): {ossf_resp.json()}\")\n",
    "                        #print(ossf_resp.json().get(\"scorecard\").get(\"overallScore\"))\n",
    "                        data[\"ossf_score\"] = ossf_resp.json().get(\"scorecard\",{}).get(\"overallScore\")\n",
    "                        cached_versions[system][package_name] = data\n",
    "                    else:\n",
    "                        LogFile.write(f\"[DEBUG] Failed to fetch OSSF score for {package_name} ({system}): {ossf_resp.status_code}\\n\")\n",
    "                        #print(f\"‚ùå Failed to fetch OSSF score for {package_name} ({system}): {ossf_resp.status_code}\")\n",
    "                        data[\"ossf_score\"] = None\n",
    "                        cached_versions[system][package_name] = data\n",
    "                else:\n",
    "                    LogFile.write(f\"[DEBUG] No related project found for {package_name} ({system})\\n\")\n",
    "                    data[\"ossf_score\"] = None\n",
    "                    cached_versions[system][package_name] = data\n",
    "                    \n",
    "                # 5) cache the combined payload\n",
    "            except requests.RequestException as e:\n",
    "                LogFile.write(f\"[DEBUG] Failed to fetch version data for {package_name} ({system}): {e}\\n\")\n",
    "                #print(f\"‚ùå Failed to fetch version data for {package_name} ({system}): {e}\")\n",
    "                cached_versions[system][package_name] = data\n",
    "\n",
    "        except (requests.RequestException, RuntimeError) as e:\n",
    "            LogFile.write(f\"[DEBUG] Failed to fetch data for {package_name} ({system}): {e}\\n\")\n",
    "            #print(f\"‚ùå Failed to fetch data for {package_name} ({system}): {e}\")\n",
    "            cached_versions[system][package_name] = {}\n",
    "\n",
    "    return cached_versions[system][package_name]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca585706",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def fetch_versions(group_id: str, artifact_id: str) -> list[str]:\n",
    "    base_url = \"https://repo1.maven.org/maven2\"\n",
    "    group_path = group_id.replace('.', '/')\n",
    "    metadata_url = f\"{base_url}/{group_path}/{artifact_id}/maven-metadata.xml\"\n",
    "\n",
    "    resp = requests.get(metadata_url)\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    root = ET.fromstring(resp.content)\n",
    "    return [v.text for v in root.findall('versioning/versions/version')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18a588ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_release_dates(group_id: str, artifact_id: str, versions: list[str]) -> dict[str, str]:\n",
    "    base_url = \"https://repo1.maven.org/maven2\"\n",
    "    group_path = group_id.replace('.', '/')\n",
    "    release_dates: dict[str, str] = {}\n",
    "\n",
    "    for version in versions:\n",
    "        pom_url = f\"{base_url}/{group_path}/{artifact_id}/{version}/{artifact_id}-{version}.pom\"\n",
    "        head = requests.head(pom_url)\n",
    "        lm = head.headers.get('Last-Modified')\n",
    "        if lm:\n",
    "            dt = datetime.strptime(lm, '%a, %d %b %Y %H:%M:%S GMT')\n",
    "            release_dates[version] = dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "        else:\n",
    "            release_dates[version] = 'unknown'\n",
    "\n",
    "    return release_dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd78d236",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_versions_with_dates_json(input_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Nimmt \"group:artifact\" als input, holt Versionen + Termine\n",
    "    und liefert einen JSON-String:\n",
    "    [\n",
    "      {\"version\": \"...\", \"released\": \"...\"},\n",
    "      ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    try:\n",
    "        group, artifact = input_str.split(\":\")\n",
    "        versions = fetch_versions(group, artifact)\n",
    "        dates = fetch_release_dates(group, artifact, versions)\n",
    "\n",
    "        # Liste von Objekten, sortiert nach Datum\n",
    "        items = {'versions': [\n",
    "            {'versionKey': {'version': v}, 'publishedAt': dates[v]}\n",
    "            for v in sorted(versions, key=lambda v: dates[v])\n",
    "        ]}\n",
    "        return json.dumps(items, indent=2)\n",
    "    except Exception as e:\n",
    "        return json.dumps({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91872884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Core analysis routine\n",
    "def analyze_entry(entry: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Given a single change entry, enrich it with:\n",
    "      - newest_version_at_commit\n",
    "      - released_date\n",
    "      - latest_at_commit_time\n",
    "      - proj_version_lag (caret‚Äêaware)\n",
    "    \"\"\"\n",
    "    pkg        = entry[\"dependency\"].split(\" \")[0].split(\"[\")[0]\n",
    "    raw_new    = entry[\"new_version\"].split(\" \")[0].split(\"[\")[0]\n",
    "    entry[\"new_version\"] = raw_new\n",
    "    this_and_above  = raw_new.startswith(\"^\") or raw_new.startswith(\"~=\") or raw_new.startswith(\">=\") or raw_new.endswith(\"*\") or raw_new == \"latest-version-available\"\n",
    "    new_ver    = raw_new.lstrip(\"^\").lstrip(\"~=\").lstrip(\">=\").lstrip(\"~<\").lstrip(\"<=\").lstrip(\"<\").lstrip(\"~\").lstrip(\">\")\n",
    "    \n",
    "    commit_dt  = datetime.strptime(entry[\"date\"], \"%Y-%m-%d %H:%M:%S\")\n",
    "    sys_name   = normalize_system(entry[\"system\"])\n",
    "    is_npm = (sys_name == \"npm\")\n",
    "    is_pypi = (sys_name == \"pypi\")\n",
    "\n",
    "\n",
    "    data       = get_cached_package_data(pkg, sys_name)\n",
    "    if data == {}:\n",
    "        data = json.loads(get_versions_with_dates_json(pkg))\n",
    "    if data == {}:\n",
    "        entry[\"alpha/beta/rc\"] = False\n",
    "        entry[\"newest_version_at_commit\"]= \"N/A\"\n",
    "        entry[\"released_date\"]= \"N/A\"\n",
    "        entry[\"latest_at_commit_time\"]= \"N/A\"\n",
    "        entry[\"proj_version_lag\"]= \"N/A\"\n",
    "        entry[\"latency\"]= \"N/A\"\n",
    "        entry[\"criticality\"]= \"N/A\"\n",
    "        return entry\n",
    "\n",
    "    #print(data)\n",
    "    all_vers   = data.get(\"versions\", [])\n",
    "    commit_vs  = []\n",
    "\n",
    "    # Filter to releases published on or before commit date\n",
    "    for v in all_vers:\n",
    "        ver = v.get(\"versionKey\", {}).get(\"version\")\n",
    "        pub = v.get(\"publishedAt\")\n",
    "        if not (ver and pub): continue\n",
    "        try:\n",
    "            pub_dt = datetime.strptime(pub, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        except ValueError:\n",
    "            continue\n",
    "        if pub_dt <= commit_dt:\n",
    "            try:\n",
    "                commit_vs.append(Version(ver))\n",
    "            except InvalidVersion:\n",
    "                pass\n",
    "        \n",
    "\n",
    "    # Determine newest version at commit time\n",
    "    entry[\"alpha/beta/rc\"] = False\n",
    "\n",
    "    if commit_vs and (is_npm or is_pypi):\n",
    "        max_major = max(v.major for v in commit_vs)\n",
    "        highest   = max(v for v in commit_vs if v.major == max_major)\n",
    "        entry[\"newest_version_at_commit\"] = str(highest)\n",
    "        entry[\"newest_version_at_commit\"] = entry[\"newest_version_at_commit\"].replace(\"a\", \"-alpha.\").replace(\"b\", \"-beta.\").replace(\"rc\", \"-rc.\")\n",
    "        entry[\"alpha/beta/rc\"] = highest.pre is not None\n",
    "        \n",
    "        #        print(f\"Highest version at commit: {highest}\")\n",
    "    else:\n",
    "        entry[\"newest_version_at_commit\"] = None\n",
    "\n",
    "    # Track release date & latest at commit\n",
    "    entry[\"released_date\"]         = None\n",
    "    entry[\"latest_at_commit_time\"] = None\n",
    "    latest_pub_date = None\n",
    "\n",
    "    target_major = get_major_version(new_ver) if is_npm else None\n",
    "\n",
    "    try:\n",
    "        baseline = Version(new_ver)\n",
    "    except InvalidVersion:\n",
    "        baseline = None\n",
    "\n",
    "    if not this_and_above:\n",
    "        for v in all_vers:\n",
    "            ver = v.get(\"versionKey\", {}).get(\"version\")\n",
    "            pub = v.get(\"publishedAt\")\n",
    "            if not (ver and pub): continue\n",
    "            try:\n",
    "                parsed = Version(ver)\n",
    "            except InvalidVersion:\n",
    "                parsed = None\n",
    "\n",
    "            # npm: restrict to same major\n",
    "            if (is_npm or is_pypi) and parsed and parsed.major != target_major:\n",
    "                continue\n",
    "\n",
    "            pub_dt = datetime.strptime(pub, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            if ver == new_ver:\n",
    "                entry[\"released_date\"] = pub_dt.isoformat()\n",
    "            if pub_dt <= commit_dt and (latest_pub_date is None or pub_dt > latest_pub_date):\n",
    "                latest_pub_date            = pub_dt\n",
    "                entry[\"latest_at_commit_time\"] = ver\n",
    "    else:\n",
    "        release_target = entry[\"newest_version_at_commit\"]\n",
    "        target_major = Version(release_target).major if release_target else None\n",
    "        for v in all_vers:\n",
    "            ver_str = v.get(\"versionKey\", {}).get(\"version\")\n",
    "            pub = v.get(\"publishedAt\")\n",
    "            if not (ver_str and pub):\n",
    "                continue\n",
    "            try:\n",
    "                pub_dt = datetime.strptime(pub, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                ver_obj = Version(ver_str)\n",
    "            except (ValueError, InvalidVersion):\n",
    "                #print(f\"Invalid version or date format: {ver_str} or {pub}\")\n",
    "                continue\n",
    "            # restrict to same major\n",
    "            if target_major is not None and ver_obj.major != target_major:\n",
    "                continue\n",
    "            if ver_str == release_target:\n",
    "                entry[\"released_date\"] = pub_dt.isoformat()\n",
    "\n",
    "    try:\n",
    "        target = Version(new_ver)\n",
    "    except InvalidVersion:\n",
    "        # handle bad new_ver here‚Ä¶\n",
    "        target = None\n",
    "\n",
    "    if entry[\"released_date\"] is None and target is not None:\n",
    "        for v in all_vers:\n",
    "            ver_str = v.get(\"versionKey\", {}).get(\"version\")\n",
    "            pub     = v.get(\"publishedAt\")\n",
    "            if not (ver_str and pub):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                ver_obj = Version(ver_str)\n",
    "                # parse your timestamp however you like‚Äî\n",
    "                # here's ISO‚Üídatetime\n",
    "                pub_dt  = datetime.strptime(pub, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            except (ValueError, InvalidVersion):\n",
    "                continue\n",
    "\n",
    "            \n",
    "\n",
    "            # compare by base_version so 1.2.3b1 == 1.2.3\n",
    "            if ver_obj.release[:len(target.release)] == target.release:\n",
    "                entry[\"released_date\"] = pub_dt.isoformat()\n",
    "                break\n",
    "    rel_date_str = entry.get(\"released_date\")\n",
    "    released_date_dt = datetime.fromisoformat(rel_date_str) if rel_date_str else None\n",
    "\n",
    "    # Compute caret‚Äêaware proj_version_lag\n",
    "    \n",
    "\n",
    "    if this_and_above and baseline:\n",
    "        same_maj = [v for v in commit_vs if v.major == baseline.major]\n",
    "        if same_maj:\n",
    "            baseline = max(same_maj)\n",
    "\n",
    "    if baseline and commit_vs:\n",
    "        entry[\"proj_version_lag\"] = sum(1 for v in commit_vs if v > baseline)\n",
    "    else:\n",
    "        entry[\"proj_version_lag\"] = -1\n",
    "        for v in all_vers:\n",
    "            version = v.get(\"versionKey\").get(\"version\")\n",
    "            v_date_str =  v.get(\"publishedAt\")\n",
    "            if not v_date_str or not version:\n",
    "                continue\n",
    "            v_date = datetime.strptime(v_date_str, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            if released_date_dt and v_date >= released_date_dt:\n",
    "                if v_date <= commit_dt:\n",
    "                    entry[\"proj_version_lag\"] += 1\n",
    "        if entry[\"proj_version_lag\"] == -1:\n",
    "            entry[\"proj_version_lag\"] = \"N/A\"\n",
    "    \n",
    "\n",
    "    # Handle latency: \n",
    "    commit_str = entry.get(\"date\")\n",
    "    commit_dt  = datetime.strptime(commit_str,  \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    release_str = entry.get(\"released_date\")\n",
    "    if release_str is None:\n",
    "        entry[\"latency\"] = \"N/A\"\n",
    "        #print(f\"Warning: No release date found for {pkg} at commit {commit_str} with version {new_ver}\")\n",
    "        return entry\n",
    "    \n",
    "\n",
    "    release_dt = datetime.strptime(release_str,  \"%Y-%m-%dT%H:%M:%S\")\n",
    "    if release_dt >= commit_dt:\n",
    "        #entry[\"latency\"] = \"future: \" + str((commit_dt - release_dt).days)\n",
    "        #entry[\"proj_version_lag\"] = \"-1\"\n",
    "        entry[\"latency\"] = \"N/A\"\n",
    "        entry[\"proj_version_lag\"] = \"N/A\"\n",
    "    else:\n",
    "        entry[\"latency\"] = (commit_dt - release_dt).days\n",
    "\n",
    "    try:\n",
    "        parsed_new = Version(new_ver)\n",
    "        entry[\"is_prerelease_new\"] = parsed_new.pre is not None\n",
    "    except InvalidVersion:\n",
    "        entry[\"is_prerelease_new\"] = False\n",
    "\n",
    "    if entry[\"new_version\"] == \"latest-version-available\":\n",
    "        entry[\"latency\"] = 0\n",
    "        entry[\"proj_version_lag\"] = 0\n",
    "\n",
    "    #print(data.get(\"ossf_score\", {}))\n",
    "    entry[\"criticality\"] = data.get(\"ossf_score\", \"N/A\")\n",
    "\n",
    "    return entry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56a5da79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Batch processing function\n",
    "def process_json_file(input_path: str, output_path: str):\n",
    "    \"\"\"\n",
    "    Read the input JSON, enrich each entry, and write to output.\n",
    "    \"\"\"\n",
    "    with open(input_path, \"r\") as f:\n",
    "        entries = json.load(f)\n",
    "\n",
    "    enriched = []\n",
    "    for i, e in enumerate(entries, 1):\n",
    "        enriched.append(analyze_entry(e))\n",
    "        #print(f\"[{i}/{len(entries)}] {e['dependency']} ‚Üí done\")\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(enriched, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d31d05-a5d0-46d3-bf8b-82b4c9b71a22",
   "metadata": {},
   "source": [
    "### 2.7 Load File at Commit\n",
    "\n",
    "Three loaders‚Äîone per file‚Äêtype‚Äîto checkout a SHA and read content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63cd0c96-a660-4c8e-b92d-41fd77e0e9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_file_at_commit_pom(repo, commit_hash, file_path):\n",
    "    try:\n",
    "        commit = repo.commit(commit_hash)\n",
    "        blob = commit.tree / file_path\n",
    "        content = blob.data_stream.read().decode('utf-8')\n",
    "        file_cache[file_path] = content\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        print(f\"[DEBUG]Error loading file '{file_path}' at commit '{commit_hash}\")    \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b600614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file_at_commit_py_js(repo_path, commit_hash, file_path):\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"git\", \"-C\", repo_path, \"show\", f\"{commit_hash}:{file_path}\"],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "        return result.stdout\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        #print(f\"Error loading file '{file_path}' at commit '{commit_hash}': {e.stderr}\")\n",
    "        try:\n",
    "            LogFile.write(f\"[DEBUG] Error loading file '{file_path}' at commit '{commit_hash}': {e.stderr}\\n\")\n",
    "            LogFile.flush()\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "614b9e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_file_at_commit_gradle(repo, commit_hash, file_path):\n",
    "    try:\n",
    "        commit = repo.commit(commit_hash)\n",
    "        blob = commit.tree / file_path\n",
    "        content = blob.data_stream.read().decode('utf-8')\n",
    "        file_cache_python[file_path] = content\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        print(f\"[DEBUG] Error loading file '{file_path}' at commit '{commit_hash}': {e}\")\n",
    "        return None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795b4e70-a8ed-40ee-8a34-301339c21129",
   "metadata": {},
   "source": [
    "### 2.8 Python/JS Parsing Helpers\n",
    "\n",
    "Split `requirements.txt`, `setup.py`, `pyproject.toml` or `package.json` into dependency dicts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ed2fc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_commit_file_content(repo_path, sha, file_path, type):\n",
    "    content = load_file_at_commit_py_js(repo_path, sha, file_path)\n",
    "        \n",
    "    if content:\n",
    "        parse_file_content(content, sha, file_path, type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48aa9a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_pyproject_toml(content: str) -> dict:\n",
    "    try:\n",
    "        data = tomli.loads(content)\n",
    "    except tomli.TOMLDecodeError:\n",
    "        return {}\n",
    "\n",
    "    # PEP 621 style\n",
    "    deps = data.get(\"project\", {}).get(\"dependencies\", [])\n",
    "    if not deps:\n",
    "        # Poetry-style\n",
    "        poetry_deps = data.get(\"tool\", {}).get(\"poetry\", {}).get(\"dependencies\", {})\n",
    "        deps = [f\"{pkg}{v if isinstance(v, str) else ''}\" for pkg, v in poetry_deps.items() if pkg.lower() != \"python\"]\n",
    "\n",
    "    result = {}\n",
    "    for dep in deps:\n",
    "        parts = re.split(r\"([<>=!~]+)\", dep, maxsplit=1)\n",
    "        name = parts[0].strip()\n",
    "        version = ''.join(parts[1:]).strip() if len(parts) > 1 else \"latest-version-available\"\n",
    "        result[name] = version\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aaa77095",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_setup_py(content: str) -> dict:\n",
    "    try:\n",
    "        tree = ast.parse(content)\n",
    "    except SyntaxError:\n",
    "        return {}\n",
    "\n",
    "    class SetupVisitor(ast.NodeVisitor):\n",
    "        def __init__(self):\n",
    "            self.install_requires = []\n",
    "\n",
    "        def visit_Call(self, node):\n",
    "            if isinstance(node.func, ast.Name) and node.func.id == \"setup\":\n",
    "                for keyword in node.keywords:\n",
    "                    if keyword.arg == \"install_requires\":\n",
    "                        if isinstance(keyword.value, (ast.List, ast.Tuple)):\n",
    "                            for elt in keyword.value.elts:\n",
    "                                if isinstance(elt, ast.Str):\n",
    "                                    self.install_requires.append(elt.s)\n",
    "            self.generic_visit(node)\n",
    "\n",
    "    visitor = SetupVisitor()\n",
    "    visitor.visit(tree)\n",
    "\n",
    "    result = {}\n",
    "    for dep in visitor.install_requires:\n",
    "        parts = re.split(r\"([<>=!~]+)\", dep, maxsplit=1)\n",
    "        name = parts[0].strip()\n",
    "        version = ''.join(parts[1:]).strip() if len(parts) > 1 else \"latest-version-available\"\n",
    "        result[name] = version\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be31456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_file_content(content, sha, filename, type):\n",
    "    dependencies = {}\n",
    "    hash_pattern = re.compile(r\"--hash=sha256:[a-fA-F0-9]{64}\")\n",
    "\n",
    "    if type == \"py\" and \"requirements.txt\" in filename:\n",
    "        for line in content.splitlines():\n",
    "            line = line.strip().split(\";\", 1)[0]  # Remove comments after semicolon\n",
    "\n",
    "            if not line or line.startswith('#') or line.startswith(\"--\"):  # Ignore comments and empty lines\n",
    "                continue\n",
    "\n",
    "            # Check if line is a hash\n",
    "            if hash_pattern.match(line):\n",
    "                continue\n",
    "\n",
    "            # Remove inline comments\n",
    "            if '#' in line:\n",
    "                line = line.split('#', 1)[0].strip()\n",
    "\n",
    "            # Find the version specifier used\n",
    "            version = \"UNSPECIFIED\"\n",
    "            for specifier in VERSION_SPECIFIERS:\n",
    "                if specifier in line:\n",
    "                    package, version = line.split(specifier, 1)\n",
    "                    package = package.strip()\n",
    "                    version = version.strip()\n",
    "                    dependencies[package] = version\n",
    "                    break\n",
    "            else:  # No version specifier found\n",
    "                package = line\n",
    "                dependencies[package] = \"latest-version-available\"\n",
    "\n",
    "    elif type == \"js\" and \"package.json\" in filename:\n",
    "        try:\n",
    "            parsed_json = json.loads(content)\n",
    "            if \"dependencies\" in parsed_json:\n",
    "                dependencies.update(parsed_json[\"dependencies\"])\n",
    "            if \"devDependencies\" in parsed_json:\n",
    "                dependencies.update(parsed_json[\"devDependencies\"])\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON file '{filename}': {e}\")\n",
    "    \n",
    "    elif type == \"py\" and \"setup.py\" in filename:\n",
    "        dependencies = parse_setup_py(content)\n",
    "    elif type == \"py\" and \"pyproject.toml\" in filename:\n",
    "        dependencies = parse_pyproject_toml(content)\n",
    "\n",
    "\n",
    "    # Store the parsed dependencies for this commit\n",
    "    dependency_changes[sha] = {\n",
    "        \"filename\": filename,\n",
    "        \"dependencies\": dependencies,\n",
    "        \"ecosystem\": type\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b02c856b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_results_python(output_file=f\"{prefix}_python_and_javascript.json\"):\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(dependency_changes, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b52955e-b960-4d12-9b5b-84965340728b",
   "metadata": {},
   "source": [
    "### 2.9 Gradle Parsing Helpers\n",
    "\n",
    "Extract Gradle `dependencies { ‚Ä¶ }` blocks, resolve property variables, group them, etc.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "62f809ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_gradle_related(file_path):\n",
    "    return (\n",
    "        file_path.endswith(\"build.gradle\")\n",
    "        or file_path.endswith(\"build.gradle.kts\")\n",
    "        or file_path.endswith(\"settings.gradle\")\n",
    "        or file_path.endswith(\"settings.gradle.kts\")\n",
    "        or file_path.endswith(\"gradle.properties\")\n",
    "        or file_path.endswith(\".gradle\")\n",
    "        or file_path.endswith(\".gradle.kts\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2ef98ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_gradle_properties(content):\n",
    "    props = {}\n",
    "    for line in content.splitlines():\n",
    "        line = line.strip()\n",
    "        if line and not line.startswith(\"#\") and \"=\" in line:\n",
    "            key, value = line.split(\"=\", 1)\n",
    "            props[key.strip()] = value.strip()\n",
    "    return props\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37d4774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_dependencies_block(content: str) -> str:\n",
    "    \"\"\"Returns only the content inside dependencies { ... } block(s).\"\"\"\n",
    "    lines = content.splitlines()\n",
    "    inside = False\n",
    "    depth = 0\n",
    "    collected = []\n",
    "\n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "        if stripped.startswith(\"dependencies\"):\n",
    "            if \"{\" in stripped:\n",
    "                inside = True\n",
    "                depth = stripped.count(\"{\") - stripped.count(\"}\")\n",
    "                continue\n",
    "\n",
    "        if inside:\n",
    "            if \"{\" in stripped:\n",
    "                depth += stripped.count(\"{\")\n",
    "            if \"}\" in stripped:\n",
    "                depth -= stripped.count(\"}\")\n",
    "            collected.append(line)\n",
    "            if depth <= 0:\n",
    "                inside = False\n",
    "\n",
    "    return \"\\n\".join(collected)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cda0f610",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_gradle_dependencies(file_content: str, properties: Dict[str, str], commit_sha: str, file_path: str) -> List[Dict[str, Any]]:\n",
    "    dependencies = []\n",
    "\n",
    "    # NEW: extract only content inside dependencies block(s)\n",
    "    block = extract_dependencies_block(file_content)\n",
    "\n",
    "    # Patterns for Kotlin + Groovy\n",
    "    patterns = [\n",
    "        re.compile(r'(?P<config>\\w+)\\s*\\(\\s*[\"\\'](?P<group>[^:\\'\"]+):(?P<artifact>[^:\\'\"]+):(?P<version>[^\\'\"]+)[\"\\']\\s*\\)'),  # Kotlin DSL\n",
    "        re.compile(r'(?P<config>\\w+)\\s+[\"\\'](?P<group>[^:\\'\"]+):(?P<artifact>[^:\\'\"]+):(?P<version>[^\\'\"]+)[\"\\']')  # Groovy DSL\n",
    "    ]\n",
    "\n",
    "    for pattern in patterns:\n",
    "        for match in pattern.finditer(block):\n",
    "            config = match.group(\"config\")\n",
    "            group = match.group(\"group\")\n",
    "            artifact = match.group(\"artifact\")\n",
    "            version = match.group(\"version\")\n",
    "\n",
    "            version_source = \"literal\"\n",
    "            if version.startswith(\"$\") or \"${\" in version:\n",
    "                var_name = version.replace(\"$\", \"\").replace(\"{\", \"\").replace(\"}\", \"\")\n",
    "                if var_name in properties:\n",
    "                    version = properties[var_name]\n",
    "\n",
    "            if \"$\" not in group and \"$\" not in artifact and \"$\" not in version:\n",
    "                dependencies.append({\n",
    "                    \"commit\": commit_sha,\n",
    "                    \"file\": file_path,\n",
    "                    #\"tool\": \"gradle\",\n",
    "                    #\"configuration\": config,\n",
    "                    \"group\": group,\n",
    "                    \"artifact\": artifact,\n",
    "                    \"version\": version,\n",
    "                    #\"version_source\": version_source\n",
    "                })\n",
    "\n",
    "    return dependencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1af135ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_local_kotlin_variables(content: str) -> Dict[str, str]:\n",
    "    props = {}\n",
    "    # Matches lines like: val log4jVersion = \"2.20.0\" or var version = \"1.0\"\n",
    "    pattern = re.compile(r'(val|var)\\s+(\\w+)\\s*=\\s*[\"\\']([^\"\\']+)[\"\\']')\n",
    "    for match in pattern.finditer(content):\n",
    "        var_name = match.group(2)\n",
    "        value = match.group(3)\n",
    "        props[var_name] = value\n",
    "    return props\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f504131",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_commit_gradle(repo, commit_hash, changed_files, dependencies_snapshot, all_dependencies, properties_by_commit):\n",
    "    for file_path in changed_files:\n",
    "        if not is_gradle_related(file_path):\n",
    "            continue\n",
    "\n",
    "        content = load_file_at_commit_gradle(repo, commit_hash, file_path)\n",
    "        if content:\n",
    "            dependencies_snapshot[file_path] = content\n",
    "\n",
    "            # Parse gradle.properties\n",
    "            if file_path.endswith(\"gradle.properties\"):\n",
    "                props = parse_gradle_properties(content)\n",
    "                properties_by_commit[commit_hash] = props\n",
    "\n",
    "            # Parse dependencies from .gradle or .gradle.kts files\n",
    "            elif file_path.endswith(\".gradle\") or file_path.endswith(\".gradle.kts\"):\n",
    "                props = {}\n",
    "                #props.update(parse_ext_block(content))                  # ext { ... }\n",
    "                #props.update(parse_extra_assignments(content))          # extra[\"...\"] = ...\n",
    "                #props.update(extract_project_properties(content))       # group/version/name\n",
    "                props.update(parse_local_kotlin_variables(content))     # val/var = \"...\"\n",
    "\n",
    "                if commit_hash not in properties_by_commit:\n",
    "                    properties_by_commit[commit_hash] = {}\n",
    "                properties_by_commit[commit_hash].update(props)\n",
    "\n",
    "                resolved_props = properties_by_commit[commit_hash]\n",
    "                parsed_deps = parse_gradle_dependencies(content, resolved_props, commit_hash, file_path)\n",
    "                all_dependencies.extend(parsed_deps)\n",
    "\n",
    "    return dependencies_snapshot, all_dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "af7a8fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_gradle_dependencies(dependencies):\n",
    "    grouped = defaultdict(lambda: {\"gradle\": defaultdict(dict)})\n",
    "\n",
    "    for dep in dependencies:\n",
    "        commit = dep[\"commit\"]\n",
    "        file = dep[\"file\"]\n",
    "        group_artifact = f\"{dep['group']}:{dep['artifact']}\"\n",
    "        version = dep[\"version\"]\n",
    "\n",
    "        grouped[commit][\"gradle\"][file][group_artifact] = version\n",
    "\n",
    "    return grouped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5dbdb7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dict(d):\n",
    "    if isinstance(d, defaultdict):\n",
    "        d = {k: to_dict(v) for k, v in d.items()}\n",
    "    elif isinstance(d, dict):\n",
    "        d = {k: to_dict(v) for k, v in d.items()}\n",
    "    return d\n",
    "\n",
    "#print(json.dumps(to_dict(grouped_dependencies), indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "db2b958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_results_gradle(data, output_file=f\"{prefix}_dependencies_over_time.json\"):\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769f01de",
   "metadata": {},
   "source": [
    "### 2.10 Merge & Clean Results\n",
    "\n",
    "Utilities to deep‚Äêmerge the three language‚Äêspecific dicts and prune empty entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "801e4099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dependencies(python_and_javascript: Dict, java: Dict, gradle: Dict) -> Dict:\n",
    "    merged_data = {}\n",
    "\n",
    "    # Combine all unique commit hashes from both datasets\n",
    "    all_commits = set(python_and_javascript) | set(java) | set(gradle)\n",
    "\n",
    "    for commit in all_commits:\n",
    "        merged_data[commit] = {\n",
    "            \"java\": {},\n",
    "            \"gradle\": {},\n",
    "            \"python\": {},\n",
    "            \"javascript\": {}\n",
    "        }\n",
    "\n",
    "        # Add Java dependencies if present\n",
    "        if commit in java:\n",
    "            for file, content in java[commit].items():\n",
    "                merged_data[commit][\"java\"][file] = content  # No renaming!\n",
    "        \n",
    "        if commit in gradle:\n",
    "            gradle_files = gradle[commit].get(\"gradle\", {})  # Extract only the inner part\n",
    "            for file, content in gradle_files.items():\n",
    "                merged_data[commit][\"gradle\"][file] = content\n",
    "\n",
    "        # Add Python and JavaScript dependencies if present\n",
    "        if commit in python_and_javascript:\n",
    "            content = python_and_javascript[commit]\n",
    "\n",
    "            # Check if it's a Python or JavaScript file based on 'ecosystem'\n",
    "            if content.get(\"ecosystem\") == \"py\":\n",
    "                merged_data[commit][\"python\"][content[\"filename\"]] = content[\"dependencies\"]\n",
    "            elif content.get(\"ecosystem\") == \"js\":\n",
    "                merged_data[commit][\"javascript\"][content[\"filename\"]] = content[\"dependencies\"]\n",
    "\n",
    "\n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "55cd6941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_objects(data):\n",
    "    if isinstance(data, dict):\n",
    "        return {\n",
    "            k: remove_empty_objects(v)\n",
    "            for k, v in data.items()\n",
    "            if not (isinstance(v, dict) and not v)  # remove empty dicts\n",
    "        }\n",
    "    elif isinstance(data, list):\n",
    "        return [remove_empty_objects(item) for item in data]\n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6315a50e",
   "metadata": {},
   "source": [
    "## 3. Execution Workflow\n",
    "\n",
    "This final section wires everything together:\n",
    "\n",
    "### 3.1 Initialize & Clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7cbbe1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sep = \"=\" * 80\n",
    "seq = \"-\" * 80\n",
    "LogFile = open(\"LOG_FILE.txt\", \"a\", encoding=\"utf-8\") \n",
    "LogFile.write(sep + \"\\n\")\n",
    "LogFile.write(f\"   Processing repository: {repo_name}\\n\")\n",
    "LogFile.write(f\"   Started at: {datetime.now().isoformat(sep=' ', timespec='seconds')}\\n\")\n",
    "LogFile.write(sep + \"\\n\\n\")\n",
    "LogFile.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "64dc3849-5b71-4b41-becb-1b27a9e344ae",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory './mitre/caldera' already exists and is not empty. Skipping clone.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(destination_path) or not os.listdir(destination_path):\n",
    "\tRepo.clone_from(repo_url, destination_path)\n",
    "else:\n",
    "\tprint(f\"Directory '{destination_path}' already exists and is not empty. Skipping clone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e4752833-4619-417d-a36d-5d6890013ab6",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "repo = Repo(destination_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599c8d30-c178-468f-8044-45bc47869ed2",
   "metadata": {},
   "source": [
    "### 3.2 Discover Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b964f07c-3839-413d-a600-b7c24ade5802",
   "metadata": {
    "editable": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "try:\n",
    "    LogFile.write(\"[INFO] LOOKING FOR FILES\")\n",
    "    LogFile.flush()\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "\n",
    "repo_path = \"./\" + repo_name  # Replace with your cloned repo path\n",
    "file_paths = find_all_files(repo_path)\n",
    "file_paths_gradle = find_all_gradle_files(repo_path)\n",
    "file_paths += file_paths_gradle\n",
    "\n",
    "\n",
    "try:\n",
    "    LogFile.write(f\"\\n[DEBUG] Found {len(file_paths)} files \\n\")\n",
    "    LogFile.flush()\n",
    "except Exception as e:\n",
    "    pass\n",
    "# Display all found pom.xml paths\n",
    "for path in file_paths:\n",
    "    path = path.replace('\\\\', '/')\n",
    "    try:\n",
    "        LogFile.write(f\"[DEBUG] Found file: {path}\\n\")\n",
    "        LogFile.flush()\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "try:\n",
    "    LogFile.write(f\"{seq}\\n\")\n",
    "    LogFile.flush()\n",
    "except Exception as e:\n",
    "    pass\n",
    "#print(len(file_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c96b00-63de-4cf0-bf9a-a7d9d0375db3",
   "metadata": {},
   "source": [
    "### 3.3 List Commits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "531e5566-d8de-44f8-bb91-5bd8089a420a",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "try:\n",
    "    LogFile.write(\"[INFO] LOOKING FOR COMMITS\\n\")\n",
    "    LogFile.flush()\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "sorted_commit_hashes = get_all_relevant_commits(repo_path, file_paths)\n",
    "\n",
    "# Display results\n",
    "#print(f\"Total number of commits: {len(sorted_commit_hashes)}\")\n",
    "#print(\"Commits (earliest to latest):\")\n",
    "\n",
    "try:\n",
    "    LogFile.write(f\"[DEBUG] Total number of commits: {len(sorted_commit_hashes)}\\n\")\n",
    "    LogFile.flush()\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "\n",
    "for commit_hash, files in list(sorted_commit_hashes.items()):  # Displaying the first 10 for brevity\n",
    "    #print(f\"{commit_hash}: {files}\")\n",
    "    try:\n",
    "        LogFile.write(f\"[DEBUG] {commit_hash}: {files}\\n\")\n",
    "        LogFile.flush()\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba8dee9-8914-4fdb-a2bd-a91281445a30",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 3.4 Parse Java-POM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "69651502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing commits: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 87/87 [00:00<00:00, 87444.15it/s]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    LogFile.write(f\"{seq}\\n\")\n",
    "    LogFile.write(f\"[INFO] PROCESSING JAVA-POM COMMITS \\n\")\n",
    "    LogFile.flush()\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "dependencies_over_time = process_commits_pom(repo, sorted_commit_hashes)\n",
    "try:\n",
    "    LogFile.write(f\"[DEBUG] Processed {len(dependencies_over_time)} commits\\n\")\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "with open(f\"{prefix}_java_pom.json\", \"w\") as f:\n",
    "    json.dump(dependencies_over_time, f, indent=4)\n",
    "\n",
    "try:\n",
    "    LogFile.write(f\"[DEBUG] Output temporarily stored in {prefix}_java_pom.json\\n\")\n",
    "    LogFile.write(f\"{seq}\\n\")\n",
    "    LogFile.flush()\n",
    "except Exception as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51ca383-1178-4511-b60a-3432b71a5ac0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 3.5 Parse Python & JS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "439fa0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize cache and data structures\n",
    "file_cache_python = {}\n",
    "unique_entries = set()\n",
    "dependency_changes = defaultdict(list)\n",
    "\n",
    "# Supported version specifiers in requirements.txt\n",
    "VERSION_SPECIFIERS = [\n",
    "    '==', '>=', '<=', '~=', '!=', '>', '<'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "77bb8729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing commits: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 87/87 [00:06<00:00, 13.39it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    LogFile.write(f\"[INFO] PROCESSING PYTHON AND JAVASCRIPT COMMITS\\n\")\n",
    "    LogFile.flush()\n",
    "except Exception as e:\n",
    "    pass\n",
    "process_commits_py_js(repo_path, sorted_commit_hashes)\n",
    "save_results_python()\n",
    "try:\n",
    "    LogFile.write(f\"[DEBUG] Processed {len(dependency_changes)} commits\\n\")\n",
    "    LogFile.write(f\"[DEBUG] Output temporarily stored in {prefix}_python_and_javascript.json\\n\")\n",
    "    LogFile.write(f\"{seq}\\n\")\n",
    "    LogFile.flush()\n",
    "except Exception as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9300fed2",
   "metadata": {},
   "source": [
    "### 3.6 Parse Java-Gradle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a954e5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing commits:   0%|          | 0/87 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing commits: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 87/87 [00:00<00:00, 86944.11it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    LogFile.write(f\"[INFO] PROCESSING JAVA-GRADLE COMMITS\\n\")\n",
    "    LogFile.flush()\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "dependencies_over_time, all_dependencies = process_commits_gradle(repo, sorted_commit_hashes)\n",
    "\n",
    "# Optionally print the parsed dependencies\n",
    "#for dep in all_dependencies:\n",
    "    #print(dep)\n",
    "\n",
    "cleaned_dependencies = [\n",
    "    dep for dep in all_dependencies\n",
    "    if \"$\" not in dep[\"group\"]\n",
    "    and \"$\" not in dep[\"artifact\"]\n",
    "    and \"$\" not in dep[\"version\"]\n",
    "]\n",
    "\n",
    "#for dep in cleaned_dependencies [:10]:\n",
    "#    print(dep)\n",
    "try:\n",
    "    LogFile.write(f\"[DEBUG] Processed {len(dependencies_over_time)} commits\\n\")\n",
    "    LogFile.flush()\n",
    "except Exception as e:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fc64d41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grouped_dependencies = group_gradle_dependencies(cleaned_dependencies)\n",
    "# Convert the defaultdict to a normal dict recursively\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f848fb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_results_gradle(to_dict(grouped_dependencies), f\"{prefix}_java_gradle.json\")\n",
    "\n",
    "try:\n",
    "    LogFile.write(f\"[DEBUG] Output temporarily stored in {prefix}_java_gradle.json\\n\")\n",
    "    LogFile.write(f\"{seq}\\n\")\n",
    "    LogFile.flush()\n",
    "except Exception as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec6843e",
   "metadata": {},
   "source": [
    "### 3.7 Merge & Save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c322ad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{prefix}_python_and_javascript.json', 'r') as f:\n",
    "    python_and_javascript = json.load(f)\n",
    "\n",
    "with open(f'{prefix}_java_pom.json', 'r') as f:\n",
    "    java = json.load(f)\n",
    "\n",
    "with open(f'{prefix}_java_gradle.json', 'r') as f:\n",
    "    gradle = json.load(f)\n",
    "\n",
    "try:\n",
    "    LogFile.write(f\"[INFO] MERGING DEPENDENCIES\\n\")\n",
    "    LogFile.flush()\n",
    "except Exception as e: \n",
    "    pass\n",
    "\n",
    "merged_data = merge_dependencies(python_and_javascript, java, gradle)\n",
    "new_merged_data = remove_empty_objects(merged_data)\n",
    "save_merged_data(new_merged_data, f'{prefix}_merged_dependencies.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1738fb",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Results & Calculated Metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0e07be4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    LogFile.write(f\"[DEBUG] Merged data saved to {prefix}_merged_dependencies.json\\n\")\n",
    "    LogFile.flush()\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "# Load input data\n",
    "with open(f\"{prefix}_merged_dependencies.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Your commit-to-date mapping\n",
    "commit_dates = commits_with_date  # assumed to be defined already\n",
    "\n",
    "# Ensure chronological commit order\n",
    "sorted_commits = sorted(data.keys(), key=lambda c: commit_dates.get(c))\n",
    "\n",
    "# Global latest version per dependency\n",
    "latest_versions = {}\n",
    "\n",
    "# Last known dependency set per file\n",
    "last_file_state = {}  # (system, filename) -> {dep: version}\n",
    "\n",
    "# All changes (add, update, remove)\n",
    "changes = []\n",
    "\n",
    "for commit in sorted_commits:\n",
    "    date = commit_dates.get(commit)\n",
    "    commit_data = data[commit]\n",
    "\n",
    "    for system, files in commit_data.items():\n",
    "        for filename, curr_deps in files.items():\n",
    "            file_key = (system, filename)\n",
    "            prev_deps = last_file_state.get(file_key, {})\n",
    "\n",
    "            # Check for added/updated\n",
    "            for dep, new_version in curr_deps.items():\n",
    "                old_version = latest_versions.get(dep)\n",
    "                if dep not in prev_deps:\n",
    "                    changes.append({\n",
    "                        \"commit\": commit,\n",
    "                        \"date\": date,\n",
    "                        \"filename\": filename,\n",
    "                        \"system\": system,\n",
    "                        \"dependency\": dep,\n",
    "                        \"change_type\": \"added\",\n",
    "                        \"old_version\": None,\n",
    "                        \"new_version\": new_version\n",
    "                    })\n",
    "                elif prev_deps[dep] != new_version:\n",
    "                    changes.append({\n",
    "                        \"commit\": commit,\n",
    "                        \"date\": date,\n",
    "                        \"filename\": filename,\n",
    "                        \"system\": system,\n",
    "                        \"dependency\": dep,\n",
    "                        \"change_type\": \"updated\",\n",
    "                        \"old_version\": prev_deps[dep].split(\" \")[0],\n",
    "                        \"new_version\": new_version.split(\" \")[0]\n",
    "                    })\n",
    "\n",
    "                # Always update the global version\n",
    "                latest_versions[dep] = new_version\n",
    "\n",
    "            # Save current state for next round\n",
    "            last_file_state[file_key] = copy.deepcopy(curr_deps)\n",
    "\n",
    "# Save all detected changes\n",
    "with open(f\"{prefix}_dependency_changes_with_removed.json\", \"w\") as f:\n",
    "    json.dump(changes, f, indent=2)\n",
    "\n",
    "try:\n",
    "    LogFile.write(f\"[DEBUG] All detected changes saved to {prefix}_dependency_changes_with_removed.json\\n\")\n",
    "    LogFile.flush()\n",
    "except Exception as e:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5dea91ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Execute!\n",
    "try:\n",
    "    LogFile.write(f\"{seq}\\n\")\n",
    "    LogFile.write(f\"[INFO] ENRICHING DEPENDENCY CHANGES\\n\")\n",
    "    LogFile.flush()\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "process_json_file(\n",
    "        f\"{prefix}_dependency_changes_with_removed.json\",\n",
    "        f\"{prefix}_dependency_changes_enriched.json\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6abbd60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    LogFile.write(f\"{seq}\\n\")\n",
    "    LogFile.write(f\"[INFO] CALCULATING METRICS\\n\")\n",
    "    LogFile.flush()\n",
    "except Exception as e:\n",
    "    pass\n",
    "result = compute_per_commit_file_metrics(f\"{prefix}_dependency_changes_enriched.json\", weeks=13)\n",
    "# Ausgabe im CSV-Format\n",
    "result.to_csv(f'{prefix}_metrics_output.csv',index=False, sep=';', encoding='utf-8')\n",
    "\n",
    "try:\n",
    "    LogFile.write(f\"[DEBUG] Metrics saved to {prefix}_metrics_output.csv\\n\")\n",
    "    LogFile.write(f\"{seq}\\n\\n\\n\\n\")\n",
    "    LogFile.flush()\n",
    "    LogFile.close()\n",
    "except Exception as e:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7fc45098-1e93-4822-bc64-74b4dad88ae9",
   "metadata": {},
   "source": [
    "p = Path('.')\n",
    "for file in p.glob(f\"{prefix}_*.json\"):\n",
    "    file.unlink()\n",
    "    print(f\"Deleted {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aa8f1663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted mitre_caldera_dependency_changes_enriched.json\n",
      "Deleted mitre_caldera_dependency_changes_with_removed.json\n",
      "Deleted mitre_caldera_java_gradle.json\n",
      "Deleted mitre_caldera_java_pom.json\n",
      "Deleted mitre_caldera_merged_dependencies.json\n",
      "Deleted mitre_caldera_python_and_javascript.json\n"
     ]
    }
   ],
   "source": [
    "p = Path('.')\n",
    "for file in p.glob(f\"{prefix}_*.json\"):\n",
    "    file.unlink()\n",
    "    print(f\"Deleted {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eab02f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bak)",
   "language": "python",
   "name": "bak"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
