{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8472f57d-b108-4d04-884d-9ad9f5a90bc9",
   "metadata": {},
   "source": [
    "# Clone Repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84d70150-836b-421b-a587-b14cb8218714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import List, Dict\n",
    "import os\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "123c0681-b26e-4325-a615-c51f5e36800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = \"intelowlproject/IntelOwl\"\n",
    "\n",
    "repo_url = \"https://github.com/\" + repo_name\n",
    "destination_path = \"./\" + repo_name\n",
    "repo_path = repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64dc3849-5b71-4b41-becb-1b27a9e344ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "GitCommandError",
     "evalue": "Cmd('git') failed due to: exit code(128)\n  cmdline: git clone -v -- https://github.com/intelowlproject/IntelOwl ./intelowlproject/IntelOwl\n  stderr: 'fatal: destination path './intelowlproject/IntelOwl' already exists and is not an empty directory.\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mGitCommandError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mRepo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclone_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m repo = Repo(destination_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\bak\\Lib\\site-packages\\git\\repo\\base.py:1541\u001b[39m, in \u001b[36mRepo.clone_from\u001b[39m\u001b[34m(cls, url, to_path, progress, env, multi_options, allow_unsafe_protocols, allow_unsafe_options, **kwargs)\u001b[39m\n\u001b[32m   1539\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1540\u001b[39m     git.update_environment(**env)\n\u001b[32m-> \u001b[39m\u001b[32m1541\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_clone\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1542\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1544\u001b[39m \u001b[43m    \u001b[49m\u001b[43mto_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1545\u001b[39m \u001b[43m    \u001b[49m\u001b[43mGitCmdObjectDB\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1546\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1547\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmulti_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1548\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unsafe_protocols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_unsafe_protocols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1549\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unsafe_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_unsafe_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1550\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\bak\\Lib\\site-packages\\git\\repo\\base.py:1412\u001b[39m, in \u001b[36mRepo._clone\u001b[39m\u001b[34m(cls, git, url, path, odb_default_type, progress, multi_options, allow_unsafe_protocols, allow_unsafe_options, **kwargs)\u001b[39m\n\u001b[32m   1409\u001b[39m     cmdline = remove_password_if_present(cmdline)\n\u001b[32m   1411\u001b[39m     _logger.debug(\u001b[33m\"\u001b[39m\u001b[33mCmd(\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms unused stdout: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, cmdline, stdout)\n\u001b[32m-> \u001b[39m\u001b[32m1412\u001b[39m     \u001b[43mfinalize_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstderr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstderr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1414\u001b[39m \u001b[38;5;66;03m# Our git command could have a different working dir than our actual\u001b[39;00m\n\u001b[32m   1415\u001b[39m \u001b[38;5;66;03m# environment, hence we prepend its working dir if required.\u001b[39;00m\n\u001b[32m   1416\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m osp.isabs(path):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\bak\\Lib\\site-packages\\git\\util.py:504\u001b[39m, in \u001b[36mfinalize_process\u001b[39m\u001b[34m(proc, **kwargs)\u001b[39m\n\u001b[32m    501\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Wait for the process (clone, fetch, pull or push) and handle its errors\u001b[39;00m\n\u001b[32m    502\u001b[39m \u001b[33;03maccordingly.\"\"\"\u001b[39;00m\n\u001b[32m    503\u001b[39m \u001b[38;5;66;03m# TODO: No close proc-streams??\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m504\u001b[39m \u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\bak\\Lib\\site-packages\\git\\cmd.py:834\u001b[39m, in \u001b[36mGit.AutoInterrupt.wait\u001b[39m\u001b[34m(self, stderr)\u001b[39m\n\u001b[32m    832\u001b[39m     errstr = read_all_from_possibly_closed_stream(p_stderr)\n\u001b[32m    833\u001b[39m     _logger.debug(\u001b[33m\"\u001b[39m\u001b[33mAutoInterrupt wait stderr: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m % (errstr,))\n\u001b[32m--> \u001b[39m\u001b[32m834\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m GitCommandError(remove_password_if_present(\u001b[38;5;28mself\u001b[39m.args), status, errstr)\n\u001b[32m    835\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m status\n",
      "\u001b[31mGitCommandError\u001b[39m: Cmd('git') failed due to: exit code(128)\n  cmdline: git clone -v -- https://github.com/intelowlproject/IntelOwl ./intelowlproject/IntelOwl\n  stderr: 'fatal: destination path './intelowlproject/IntelOwl' already exists and is not an empty directory.\n'"
     ]
    }
   ],
   "source": [
    "Repo.clone_from(repo_url, destination_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4752833-4619-417d-a36d-5d6890013ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = Repo(destination_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d56039a9-fe48-4c56-9833-7e77586a1ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_files(repo_path):\n",
    "    pom_files = []\n",
    "    for root, dirs, files in os.walk(repo_path):\n",
    "        for file in files:\n",
    "            if file == \"pom.xml\" or file == \"requirements.txt\" or file ==\"package.json\":\n",
    "                full_path = os.path.join(root, file)\n",
    "                pom_files.append(full_path)\n",
    "    return pom_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ec8c921-0a7e-41e0-bb9e-6b0aae92c80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_commits_with_changes(repo_path, file_paths):\n",
    "    repo = Repo(repo_path)\n",
    "    commits_with_changes = {}\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        rel_path = os.path.relpath(file_path, repo_path)\n",
    "        commits = list(repo.iter_commits(paths=rel_path))\n",
    "        \n",
    "        commits_with_changes[rel_path] = [commit.hexsha for commit in commits]\n",
    "    \n",
    "    return commits_with_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b964f07c-3839-413d-a600-b7c24ade5802",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./intelowlproject/IntelOwl\\frontend\\package.json\n",
      "./intelowlproject/IntelOwl\\integrations\\nuclei_analyzer\\requirements.txt\n",
      "./intelowlproject/IntelOwl\\integrations\\pcap_analyzers\\requirements.txt\n",
      "./intelowlproject/IntelOwl\\integrations\\phishing_analyzers\\requirements.txt\n",
      "./intelowlproject/IntelOwl\\integrations\\thug\\requirements.txt\n",
      "./intelowlproject/IntelOwl\\integrations\\tor_analyzers\\requirements.txt\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "repo_path = \"./\" + repo_name  # Replace with your cloned repo path\n",
    "file_paths = find_all_files(repo_path)\n",
    "\n",
    "# Display all found pom.xml paths\n",
    "for path in file_paths:\n",
    "    print(path)\n",
    "\n",
    "print(len(file_paths))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee22db3a-d6b8-4f55-8c2a-587c405e3c9b",
   "metadata": {},
   "source": [
    "commits_with_changes = find_commits_with_changes(repo_path, file_paths)\n",
    "\n",
    "# Display results\n",
    "for file_path, commits in commits_with_changes.items():g\n",
    "    print(f\"Commits modifying {file_path}:\")\n",
    "    for commit in commits:\n",
    "        print(f\" - {commit}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "21602d1a-57dd-46c9-aca2-2756e3ac1128",
   "metadata": {},
   "source": [
    "# Calculate the total number of commits\n",
    "total_commits = sum(len(commits) for commits in commits_with_changes.values())\n",
    "print(f\"Total number of commits affecting all pom.xml files: {total_commits}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d8cd0a8-7fd8-420f-bfdf-cacc4fbbfdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of commits: 122\n",
      "Commits (earliest to latest):\n",
      "4ca770ba9f2aa37f47d1d79816cb8196982f65e3: ['integrations/peframe/requirements.txt']\n",
      "d8c20f24b56cf7aafa18ccff9e577b3d72ba959b: ['integrations/peframe/requirements.txt']\n",
      "9715a01c6032f78a3bef6c5d6e8ad922ab79491e: ['integrations/thug/requirements.txt']\n",
      "3a0cb6e5f5a8db1e7f3e5a09291e402ee8ad2a4d: ['integrations/thug/requirements.txt', 'integrations/peframe/requirements.txt']\n",
      "6aaa418fdc7845d1dc0adf4f485d217e62e027c5: ['integrations/box-js/requirements.txt']\n",
      "c4e8f88327fc829d47c02217bf6ec4bfa9c8d025: ['integrations/capa/requirements.txt']\n",
      "625c1c14623dbf0b800c94dc4ac93f90dd514a32: ['integrations/thug/requirements.txt', 'integrations/box-js/requirements.txt']\n",
      "77898109061ffa186eaf3d7033088335aa930635: ['integrations/apk_analyzers/requirements.txt']\n",
      "74878c7636d573819825834544bd5547a1a4c2e0: ['integrations/thug/requirements.txt', 'integrations/apk_analyzers/requirements.txt']\n",
      "ee0627edb006b1ec083cbadb82251415c1d17a23: ['integrations/thug/requirements.txt', 'integrations/apk_analyzers/requirements.txt']\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "from git import Repo\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def get_all_relevant_commits(repo_path: str, file_paths: List[str]) -> Dict[str, List[str]]:\n",
    "    repo = Repo(repo_path)\n",
    "    commits_with_changes = {}\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        rel_path = os.path.relpath(file_path, repo_path)\n",
    "\n",
    "        # Get all commits that modified the file\n",
    "        result = subprocess.run(\n",
    "            [\"git\", \"-C\", repo_path, \"log\", \"--follow\", \"--pretty=format:%H\", \"--name-only\", \"--\", rel_path],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "\n",
    "        lines = result.stdout.strip().split('\\n')\n",
    "        current_commit = None\n",
    "\n",
    "        for line in lines:\n",
    "            if len(line) == 40:  # SHA-1 hash length\n",
    "                current_commit = line.strip()\n",
    "                if current_commit not in commits_with_changes:\n",
    "                    commits_with_changes[current_commit] = set()  # Using set to avoid duplicates\n",
    "            elif current_commit:\n",
    "                modified_file = line.strip()\n",
    "                if modified_file:  # Ensure we don't add empty lines\n",
    "                    commits_with_changes[current_commit].add(modified_file)\n",
    "\n",
    "    # Convert commits to a sorted list based on commit date\n",
    "    commit_objects = [(repo.commit(commit_hash), commit_hash) for commit_hash in commits_with_changes.keys()]\n",
    "    sorted_commits = sorted(commit_objects, key=lambda x: x[0].committed_date)\n",
    "    \n",
    "    # Creating a sorted dictionary of commits with their modified files\n",
    "    sorted_commits_with_changes = {commit_hash: list(commits_with_changes[commit_hash]) for _, commit_hash in sorted_commits}\n",
    "\n",
    "    return sorted_commits_with_changes\n",
    "\n",
    "\n",
    "# Example usage\n",
    "sorted_commit_hashes = get_all_relevant_commits(repo_path, file_paths)\n",
    "\n",
    "# Display results\n",
    "print(f\"Total number of commits: {len(sorted_commit_hashes)}\")\n",
    "print(\"Commits (earliest to latest):\")\n",
    "for commit_hash, files in list(sorted_commit_hashes.items())[:10]:  # Displaying the first 10 for brevity\n",
    "    print(f\"{commit_hash}: {files}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9ea4990-e85e-4875-9eb5-320c70b69d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading file 'integrations/thug/requirements.txt' at commit '04ed7af5fd10dff0d163e6975939ae0842253f3d': fatal: path 'integrations/thug/requirements.txt' exists on disk, but not in '04ed7af5fd10dff0d163e6975939ae0842253f3d'\n",
      "\n",
      "Error loading file 'integrations/apk_analyzers/requirements.txt' at commit '04ed7af5fd10dff0d163e6975939ae0842253f3d': fatal: path 'integrations/apk_analyzers/requirements.txt' does not exist in '04ed7af5fd10dff0d163e6975939ae0842253f3d'\n",
      "\n",
      "Error loading file 'integrations/thug/requirements.txt' at commit '0035119b3610aa52dfe1846c82fe14a0feff142c': fatal: path 'integrations/thug/requirements.txt' exists on disk, but not in '0035119b3610aa52dfe1846c82fe14a0feff142c'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import requests\n",
    "\n",
    "# Initialize cache and data structures\n",
    "file_cache = {}\n",
    "unique_entries = set()\n",
    "dependency_changes = defaultdict(list)\n",
    "\n",
    "# Supported version specifiers in requirements.txt\n",
    "VERSION_SPECIFIERS = [\n",
    "    '==', '>=', '<=', '~=', '!=', '>', '<'\n",
    "]\n",
    "\n",
    "\n",
    "def load_file_at_commit(repo_path, commit_hash, file_path):\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"git\", \"-C\", repo_path, \"show\", f\"{commit_hash}:{file_path}\"],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "        return result.stdout\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error loading file '{file_path}' at commit '{commit_hash}': {e.stderr}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_commit_file_content(repo_path, sha, file_path, type):\n",
    "    content = load_file_at_commit(repo_path, sha, file_path)\n",
    "\n",
    "    if content:\n",
    "        parse_file_content(content, sha, file_path, type)\n",
    "\n",
    "\n",
    "def parse_file_content(content, sha, filename, type):\n",
    "    dependencies = {}\n",
    "    hash_pattern = re.compile(r\"--hash=sha256:[a-fA-F0-9]{64}\")\n",
    "\n",
    "    if type == \"py\" and \"requirements.txt\" in filename:\n",
    "        for line in content.splitlines():\n",
    "            line = line.strip()\n",
    "\n",
    "            if not line or line.startswith('#') or line.startswith(\"--\"):  # Ignore comments and empty lines\n",
    "                continue\n",
    "\n",
    "            # Check if line is a hash\n",
    "            if hash_pattern.match(line):\n",
    "                continue\n",
    "\n",
    "            # Remove inline comments\n",
    "            if '#' in line:\n",
    "                line = line.split('#', 1)[0].strip()\n",
    "\n",
    "            # Find the version specifier used\n",
    "            version = \"UNSPECIFIED\"\n",
    "            for specifier in VERSION_SPECIFIERS:\n",
    "                if specifier in line:\n",
    "                    package, version = line.split(specifier, 1)\n",
    "                    package = package.strip()\n",
    "                    version = version.strip()\n",
    "                    dependencies[package] = version\n",
    "                    break\n",
    "            else:  # No version specifier found\n",
    "                package = line\n",
    "                dependencies[package] = \"latest-version-available\"\n",
    "\n",
    "    elif type == \"js\" and \"package.json\" in filename:\n",
    "        import json\n",
    "        try:\n",
    "            parsed_json = json.loads(content)\n",
    "            if \"dependencies\" in parsed_json:\n",
    "                dependencies.update(parsed_json[\"dependencies\"])\n",
    "            if \"devDependencies\" in parsed_json:\n",
    "                dependencies.update(parsed_json[\"devDependencies\"])\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON file '{filename}': {e}\")\n",
    "\n",
    "    # Store the parsed dependencies for this commit\n",
    "    dependency_changes[sha] = {\n",
    "        \"filename\": filename,\n",
    "        \"dependencies\": dependencies,\n",
    "        \"ecosystem\": type\n",
    "    }\n",
    "\n",
    "\n",
    "def process_commits(repo_path, commits_with_files):\n",
    "    for sha, files in commits_with_files.items():\n",
    "        for file_path in files:\n",
    "            if file_path.endswith(\"requirements.txt\"):\n",
    "                process_commit_file_content(repo_path, sha, file_path, \"py\")\n",
    "            elif file_path.endswith(\"package.json\"):\n",
    "                process_commit_file_content(repo_path, sha, file_path, \"js\")\n",
    "\n",
    "\n",
    "def save_results(output_file=\"dependencies_over_time_py.json\"):\n",
    "    import json\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(dependency_changes, f, indent=4)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# repo_path = '/path/to/your/local/repo'\n",
    "# commits_with_files = {\n",
    "#     '1726bfbdcf08fdf2fde2517545bfbd2ee36095e': ['package.json'],\n",
    "#     '2f1798417e3ee1e1d589242cd61cf52ab09e8864': ['package.json']\n",
    "# }\n",
    "process_commits(repo_path, sorted_commit_hashes)\n",
    "save_results()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2410a91-14f3-4a98-906b-f0dd339aeac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing commits: 100%|██████████| 122/122 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize file cache\n",
    "file_cache = {}\n",
    "\n",
    "\n",
    "NAMESPACE = {'mvn': 'http://maven.apache.org/POM/4.0.0'}\n",
    "\n",
    "\n",
    "def parse_pom_xml(content: str, properties: Dict[str, str] = None) -> Dict[str, str]:\n",
    "    if properties is None:\n",
    "        properties = {}\n",
    "    dependencies = {}\n",
    "\n",
    "    try:\n",
    "        root = ET.fromstring(content)\n",
    "\n",
    "        # Register namespace if needed\n",
    "        if 'xmlns' in root.attrib:\n",
    "            namespace_uri = root.attrib['xmlns']\n",
    "            NAMESPACE['mvn'] = namespace_uri\n",
    "\n",
    "        # Load project properties\n",
    "        for prop in root.findall(\".//mvn:properties/*\", NAMESPACE):\n",
    "            if prop.tag and prop.text:\n",
    "                prop_name = prop.tag.split('}')[-1]  # Remove namespace part\n",
    "                properties[prop_name] = prop.text.strip()\n",
    "\n",
    "        project_version = root.find(\".//mvn:version\", NAMESPACE)\n",
    "        if project_version is not None and project_version.text:\n",
    "            properties[\"project.version\"] = project_version.text.strip()\n",
    "\n",
    "        parent_version = root.find(\".//mvn:parent/mvn:version\", NAMESPACE)\n",
    "        if parent_version is not None and parent_version.text:\n",
    "            properties[\"parent.version\"] = parent_version.text.strip()\n",
    "\n",
    "        for dependency in root.findall(\".//mvn:dependency\", NAMESPACE):\n",
    "            group_id = dependency.find(\"mvn:groupId\", NAMESPACE)\n",
    "            artifact_id = dependency.find(\"mvn:artifactId\", NAMESPACE)\n",
    "            version = dependency.find(\"mvn:version\", NAMESPACE)\n",
    "\n",
    "            if group_id is not None and artifact_id is not None:\n",
    "                group_id = group_id.text.strip()\n",
    "                artifact_id = artifact_id.text.strip()\n",
    "                dep_key = f\"{group_id}:{artifact_id}\"\n",
    "\n",
    "                if version is not None:\n",
    "                    version_text = version.text.strip()\n",
    "\n",
    "                    if version_text.startswith(\"${\") and version_text.endswith(\"}\"):\n",
    "                        prop_name = version_text[2:-1]\n",
    "                        resolved_version = properties.get(prop_name, \"UNRESOLVED\")\n",
    "                        if resolved_version == \"UNRESOLVED\":\n",
    "                            resolved_version = resolve_from_parent(dep_key, properties)\n",
    "                        dependencies[dep_key] = resolved_version\n",
    "                    else:\n",
    "                        dependencies[dep_key] = version_text\n",
    "\n",
    "    except ET.ParseError as e:\n",
    "        print(f\"XML parsing error: {e}\")\n",
    "\n",
    "    return dependencies\n",
    "\n",
    "\n",
    "def resolve_from_parent(dep_key: str, properties: Dict[str, str]) -> str:\n",
    "    parent_pom_path = properties.get(\"parent_pom_path\")\n",
    "    if parent_pom_path and os.path.exists(parent_pom_path):\n",
    "        with open(parent_pom_path, \"r\") as f:\n",
    "            parent_content = f.read()\n",
    "            parent_properties = parse_pom_xml(parent_content, properties)\n",
    "            return parent_properties.get(dep_key, \"UNRESOLVED\")\n",
    "    return \"UNRESOLVED\"\n",
    "\n",
    "\n",
    "def load_file_at_commit(repo, commit_hash, file_path):\n",
    "    cache_key = (commit_hash, file_path)\n",
    "    if cache_key in file_cache:\n",
    "        return file_cache[cache_key]\n",
    "\n",
    "    try:\n",
    "        commit = repo.commit(commit_hash)\n",
    "        blob = commit.tree / file_path\n",
    "        content = blob.data_stream.read().decode('utf-8')\n",
    "        file_cache[cache_key] = content\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        file_cache[cache_key] = None\n",
    "        print(f\"Error loading file '{file_path}' at commit '{commit_hash}': {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_commit(repo, commit_hash, changed_files, dependencies_snapshot):\n",
    "    for file_path in changed_files:\n",
    "        if file_path.endswith(\"pom.xml\"):\n",
    "            content = load_file_at_commit(repo, commit_hash, file_path)\n",
    "            if content:\n",
    "                properties = {\"parent_pom_path\": \"../pom.xml\"}  # Provide the parent path if known\n",
    "                new_dependencies = parse_pom_xml(content, properties)\n",
    "                dependencies_snapshot[file_path] = new_dependencies\n",
    "    return dependencies_snapshot\n",
    "\n",
    "\n",
    "def process_commits(repo, commits_with_changes):\n",
    "    dependencies_over_time = {}\n",
    "    dependencies_snapshot = {}\n",
    "\n",
    "    for commit_hash, changed_files in tqdm(commits_with_changes.items(), desc=\"Processing commits\"):\n",
    "        dependencies_snapshot = process_commit(repo, commit_hash, changed_files, dependencies_snapshot)\n",
    "        dependencies_over_time[commit_hash] = dependencies_snapshot.copy()\n",
    "\n",
    "    return dependencies_over_time\n",
    "\n",
    "\n",
    "dependencies_over_time = process_commits(repo, sorted_commit_hashes)\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"dependencies_over_time.json\", \"w\") as f:\n",
    "    json.dump(dependencies_over_time, f, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
