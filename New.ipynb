{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8472f57d-b108-4d04-884d-9ad9f5a90bc9",
   "metadata": {},
   "source": [
    "## Clone Repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84d70150-836b-421b-a587-b14cb8218714",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import List, Dict\n",
    "from collections import defaultdict\n",
    "from git import Repo, BadName\n",
    "from pathlib import Path\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "123c0681-b26e-4325-a615-c51f5e36800a",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "repo_name = \"zaproxy/zaproxy\"\n",
    "\n",
    "repo_url = \"https://github.com/\" + repo_name\n",
    "destination_path = \"./\" + repo_name\n",
    "repo_path = repo_name\n",
    "\n",
    "commits_with_date = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64dc3849-5b71-4b41-becb-1b27a9e344ae",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory './zaproxy/zaproxy' already exists and is not empty. Skipping clone.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(destination_path) or not os.listdir(destination_path):\n",
    "\tRepo.clone_from(repo_url, destination_path)\n",
    "else:\n",
    "\tprint(f\"Directory '{destination_path}' already exists and is not empty. Skipping clone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4752833-4619-417d-a36d-5d6890013ab6",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "repo = Repo(destination_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d56039a9-fe48-4c56-9833-7e77586a1ec6",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_all_files(repo_path):\n",
    "    found_files = []\n",
    "    for root, dirs, file_list in os.walk(repo_path):  # renamed 'files' to 'file_list'\n",
    "        for file in file_list:\n",
    "            if file == \"pom.xml\" or file == \"requirements.txt\" or file == \"package.json\":\n",
    "                full_path = os.path.join(root, file)\n",
    "                found_files.append(full_path)\n",
    "    return found_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c788b6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "\n",
    "\n",
    "def find_all_gradle_files(repo_path):\n",
    "    gradle_files = []\n",
    "    patterns = [\n",
    "        \"build.gradle\",\n",
    "        \"settings.gradle\",\n",
    "        \"gradle.properties\",\n",
    "        \"build.gradle.kts\",\n",
    "        \"settings.gradle.kts\",\n",
    "        \"*.gradle\",\n",
    "        \"*.gradle.kts\"\n",
    "    ]\n",
    "\n",
    "    for root, dirs, files in os.walk(repo_path):\n",
    "        for file in files:\n",
    "            for pattern in patterns:\n",
    "                if fnmatch.fnmatch(file, pattern):\n",
    "                    full_path = os.path.join(root, file)\n",
    "                    gradle_files.append(full_path)\n",
    "                    break\n",
    "    return gradle_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ec8c921-0a7e-41e0-bb9e-6b0aae92c80d",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_commits_with_changes(repo_path, file_paths):\n",
    "    repo = Repo(repo_path)\n",
    "    commits_with_changes = {}\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        rel_path = os.path.relpath(file_path, repo_path)\n",
    "        commits = list(repo.iter_commits(paths=rel_path))\n",
    "        \n",
    "        commits_with_changes[rel_path] = [commit.hexsha for commit in commits]\n",
    "    \n",
    "    return commits_with_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b964f07c-3839-413d-a600-b7c24ade5802",
   "metadata": {
    "editable": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./zaproxy/zaproxy/docker/requirements.txt\n",
      "./zaproxy/zaproxy/build.gradle.kts\n",
      "./zaproxy/zaproxy/gradle.properties\n",
      "./zaproxy/zaproxy/settings.gradle.kts\n",
      "./zaproxy/zaproxy/buildSrc/build.gradle.kts\n",
      "./zaproxy/zaproxy/buildSrc/settings.gradle.kts\n",
      "./zaproxy/zaproxy/buildSrc/src/main/kotlin/org/zaproxy/zap/distributions.gradle.kts\n",
      "./zaproxy/zaproxy/buildSrc/src/main/kotlin/org/zaproxy/zap/github-releases.gradle.kts\n",
      "./zaproxy/zaproxy/buildSrc/src/main/kotlin/org/zaproxy/zap/installers.gradle.kts\n",
      "./zaproxy/zaproxy/buildSrc/src/main/kotlin/org/zaproxy/zap/jflex.gradle.kts\n",
      "./zaproxy/zaproxy/buildSrc/src/main/kotlin/org/zaproxy/zap/publish.gradle.kts\n",
      "./zaproxy/zaproxy/buildSrc/src/main/kotlin/org/zaproxy/zap/spotless.gradle.kts\n",
      "./zaproxy/zaproxy/buildSrc/src/main/kotlin/org/zaproxy/zap/test.gradle.kts\n",
      "./zaproxy/zaproxy/gradle/ci.gradle.kts\n",
      "./zaproxy/zaproxy/zap/gradle.properties\n",
      "./zaproxy/zaproxy/zap/zap.gradle.kts\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "repo_path = \"./\" + repo_name  # Replace with your cloned repo path\n",
    "file_paths = find_all_files(repo_path)\n",
    "file_paths_gradle = find_all_gradle_files(repo_path)\n",
    "file_paths += file_paths_gradle\n",
    "\n",
    "# Display all found pom.xml paths\n",
    "for path in file_paths:\n",
    "    path = path.replace('\\\\', '/')\n",
    "    print(path)\n",
    "\n",
    "print(len(file_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d8cd0a8-7fd8-420f-bfdf-cacc4fbbfdb3",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_all_relevant_commits(repo_path: str, file_paths: List[str]) -> Dict[str, List[str]]:\n",
    "    repo = Repo(repo_path)\n",
    "    commits_with_changes = {}\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        rel_path = os.path.relpath(file_path, repo_path)\n",
    "\n",
    "        # Get all commits that modified the file\n",
    "        result = subprocess.run(\n",
    "            [\"git\", \"-C\", repo_path, \"log\", \"--follow\", \"--pretty=format:%H\", \"--name-only\", \"--\", rel_path],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "\n",
    "        lines = result.stdout.strip().split('\\n')\n",
    "        current_commit = None\n",
    "\n",
    "        for line in lines:\n",
    "            stripped_line = line.strip()\n",
    "            \n",
    "            if len(stripped_line) == 40 and all(c in '0123456789abcdef' for c in stripped_line):  # Check if it looks like a SHA-1 hash\n",
    "                current_commit = stripped_line\n",
    "                if current_commit not in commits_with_changes:\n",
    "                    commits_with_changes[current_commit] = set()  # Using set to avoid duplicates\n",
    "            elif current_commit:\n",
    "                modified_file = stripped_line\n",
    "                if modified_file:  # Ensure we don't add empty lines\n",
    "                    commits_with_changes[current_commit].add(modified_file)\n",
    "\n",
    "    # Convert commits to a sorted list based on commit date\n",
    "    commit_objects = []\n",
    "    for commit_hash in commits_with_changes.keys():\n",
    "        try:\n",
    "            commit_obj = repo.commit(commit_hash)\n",
    "            commit_objects.append((commit_obj, commit_hash))\n",
    "        except BadName:\n",
    "            print(f\"Skipping invalid commit hash: {commit_hash}\")  # Warn if a bad commit hash is found\n",
    "\n",
    "    sorted_commits = sorted(commit_objects, key=lambda x: x[0].committed_date)\n",
    "    \n",
    "    # Creating a sorted dictionary of commits with their modified files\n",
    "    sorted_commits_with_changes = {\n",
    "        commit_hash: list(commits_with_changes[commit_hash]) for _, commit_hash in sorted_commits\n",
    "        }\n",
    "    \n",
    "    global commits_with_date\n",
    "    commits_with_date = {\n",
    "        commit_hash: repo.commit(commit_hash).committed_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        for _, commit_hash in sorted_commits\n",
    "    }\n",
    "\n",
    "    return sorted_commits_with_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "531e5566-d8de-44f8-bb91-5bd8089a420a",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of commits: 276\n",
      "Commits (earliest to latest):\n",
      "48696324116df2c5006633a42f12a8ce17632911: ['src/test']\n",
      "ed87c458b4963c8151eac40cdb4393d4dc83b840: ['src/log/dummy.txt']\n",
      "f573818d36382d3157b5a30cdaf22fc07ebcf3aa: ['src/fuzzers/dummy.txt']\n",
      "a8672647f6a300c585e609a2eeb1a3b91339a8cf: ['test/resources/vulnerabilities/invalid/vulnerabilities-test-empty.xml']\n",
      "fe1131cc1383ce1c369b95899c6fd0fd36452b55: ['test/resources/org/zaproxy/zap/spider/parser/sitemapxml/EmptyFile.xml']\n",
      "8cc30d8daec20df2f5dc35563d5886e32b651b1c: ['build/docker/requirements.txt']\n",
      "efb404d38280dc9ecf8f88c9b0c658385861bdcf: ['docker/requirements.txt']\n",
      "d0c70dc72ba0268fe6f7111715370c99c63a0245: ['gradle.properties', 'gradle/travis-ci.gradle.kts', 'build.gradle.kts', 'settings.gradle.kts']\n",
      "83e1f04a7e59283583bc6527018f74312ece12da: ['zap/zap.gradle.kts', 'settings.gradle.kts']\n",
      "8960befa37a1e5e04b80cb044aa5121af115dd43: ['zap/zap.gradle.kts']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "sorted_commit_hashes = get_all_relevant_commits(repo_path, file_paths)\n",
    "\n",
    "# Display results\n",
    "print(f\"Total number of commits: {len(sorted_commit_hashes)}\")\n",
    "print(\"Commits (earliest to latest):\")\n",
    "for commit_hash, files in list(sorted_commit_hashes.items())[:10]:  # Displaying the first 10 for brevity\n",
    "    print(f\"{commit_hash}: {files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba8dee9-8914-4fdb-a2bd-a91281445a30",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## java pom.xml parsing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83538dc7-84f6-4bec-9fba-dd3cffcca969",
   "metadata": {
    "editable": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing commits: 100%|██████████| 276/276 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize file cache\n",
    "file_cache = {}\n",
    "properties_cache = {}\n",
    "NAMESPACE = {'mvn': 'http://maven.apache.org/POM/4.0.0'}\n",
    "properties = {}\n",
    "\n",
    "def parse_pom_xml(content: str, file_path: str, properties: Dict[str, str] = None) -> Dict[str, str]:\n",
    "    if properties is None:\n",
    "        properties = {}\n",
    "    else:\n",
    "        properties = properties.copy()  # Avoid side effects\n",
    "\n",
    "    dependencies = {}\n",
    "\n",
    "    try:\n",
    "        root = ET.fromstring(content)\n",
    "\n",
    "        # Update namespace if needed\n",
    "        if 'xmlns' in root.attrib:\n",
    "            NAMESPACE['mvn'] = root.attrib['xmlns']\n",
    "\n",
    "        # Load project properties\n",
    "        for prop in root.findall(\".//mvn:properties/*\", NAMESPACE):\n",
    "            if prop.tag and prop.text:\n",
    "                prop_name = prop.tag.split('}')[-1]\n",
    "                properties[prop_name] = prop.text.strip()\n",
    "                properties_cache[prop_name] = properties[prop_name]\n",
    "        \n",
    "        # Load project and parent versions as fallback properties\n",
    "        project_version = root.find(\".//mvn:version\", NAMESPACE)\n",
    "        if project_version is not None and project_version.text:\n",
    "            properties[\"project.version\"] = project_version.text.strip()\n",
    "\n",
    "        parent_version = root.find(\".//mvn:parent/mvn:version\", NAMESPACE)\n",
    "        if parent_version is not None and parent_version.text:\n",
    "            properties[\"parent.version\"] = parent_version.text.strip()\n",
    "\n",
    "        parent_info = {}\n",
    "        relative_path_elem = root.find(\".//mvn:parent/mvn:relativePath\", NAMESPACE)\n",
    "        if relative_path_elem is not None and relative_path_elem.text:\n",
    "            parent_info = {\"parent_pom_path\": relative_path_elem.text.strip()}\n",
    "\n",
    "        # Read dependencies\n",
    "        for dependency in root.findall(\".//mvn:dependency\", NAMESPACE):\n",
    "            group_id = dependency.find(\"mvn:groupId\", NAMESPACE)\n",
    "            artifact_id = dependency.find(\"mvn:artifactId\", NAMESPACE)\n",
    "            version = dependency.find(\"mvn:version\", NAMESPACE)\n",
    "\n",
    "            if group_id is not None and artifact_id is not None:\n",
    "                dep_key = f\"{group_id.text.strip()}:{artifact_id.text.strip()}\"\n",
    "                if \"${\" in dep_key and \"}\" in dep_key:\n",
    "                    dep_key = (resolve_cached(dep_key))\n",
    "\n",
    "                if version is not None and version.text:\n",
    "                    version_text = version.text.strip()\n",
    "                    if version_text.startswith(\"${\") and version_text.endswith(\"}\"):\n",
    "                        prop_name = version_text[2:-1]\n",
    "                        resolved_version = properties.get(prop_name, \"UNRESOLVED\")\n",
    "                        if resolved_version == \"UNRESOLVED\":\n",
    "                            resolved_version = resolve_from_parent(prop_name, file_path, parent_info)\n",
    "                            if (resolved_version ==\"UNRESOLVED\"):\n",
    "                                resolved_version = properties_cache.get(prop_name, \"UNRESOLVED\")\n",
    "                        if resolved_version.startswith(\"${\") and resolved_version.endswith(\"}\"):\n",
    "                            resolved_version = resolve_cached(resolved_version)\n",
    "                        dependencies[dep_key] = resolved_version\n",
    "                    else:\n",
    "                        dependencies[dep_key] = version_text\n",
    "                #else:\n",
    "                    #dependencies[dep_key] = \"UNSPECIFIED\"\n",
    "\n",
    "    except ET.ParseError as e:\n",
    "        print(f\"XML parsing error: {e}\")\n",
    "\n",
    "    return dependencies\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def resolve_cached(prop_name, visited=None):\n",
    "    matches = re.findall(r\"\\$\\{([^}]+)\\}\", prop_name)\n",
    "    if not matches:\n",
    "        return properties_cache.get(prop_name)  # nothing to resolve, return as is\n",
    "\n",
    "    resolved = prop_name\n",
    "    for match in matches:\n",
    "        inner_value = resolve_cached(match)\n",
    "        if inner_value is None:\n",
    "            return match\n",
    "        resolved = resolved.replace(f\"${{{match}}}\", inner_value)\n",
    "\n",
    "    return resolved\n",
    "\n",
    "\n",
    "\n",
    "def resolve_from_parent(prop_name: str, file_path: str, properties: Dict[str, str]) -> str:\n",
    "    parent_pom_path = properties.get(\"parent_pom_path\")   \n",
    "    if parent_pom_path is None:\n",
    "        final_path = \"pom.xml\"\n",
    "        parent_file = str(final_path).replace(\"\\\\\", \"/\")\n",
    "        if parent_file not in file_cache:\n",
    "            return \"UNRESOLVED\"\n",
    "        content = file_cache[parent_file]\n",
    "        parent_prop_value = get_all_properties(content, prop_name, final_path, {})\n",
    "        return parent_prop_value\n",
    "\n",
    "    file_path = Path(file_path) if not isinstance(file_path, Path) else file_path\n",
    "    parent_pom_path = Path(parent_pom_path) if not isinstance(parent_pom_path, Path) else parent_pom_path\n",
    "    combined = file_path.parent / parent_pom_path\n",
    "\n",
    "    stack = []\n",
    "    for part in combined.parts:\n",
    "        if part == \"..\":\n",
    "            if stack and stack[-1] != \"..\":\n",
    "                stack.pop()\n",
    "            else:\n",
    "                stack.append(part)\n",
    "        elif part != \".\":\n",
    "            stack.append(part)\n",
    "    final_path = Path(*stack)\n",
    "    parent_file = str(final_path).replace(\"\\\\\", \"/\")\n",
    "    if parent_file not in file_cache:\n",
    "        #print(f\"[DEBUG] Skipping missing file: {parent_file}\")\n",
    "        return \"UNRESOLVED\"\n",
    "    content = file_cache[parent_file]\n",
    "    parent_prop_value = get_all_properties(content, prop_name, final_path, {})\n",
    "    return parent_prop_value\n",
    "\n",
    "def get_all_properties(content: str, target_prop: str, file_path: str, properties: Dict[str, str] = None):\n",
    "    if properties is None:\n",
    "        properties = {}\n",
    "    else:\n",
    "        properties = properties.copy()  # Avoid side effects\n",
    "\n",
    "    try:\n",
    "        root = ET.fromstring(content)\n",
    "\n",
    "        if 'xmlns' in root.attrib:\n",
    "            NAMESPACE['mvn'] = root.attrib['xmlns']\n",
    "\n",
    "        # Properly assign keys without shadowing target_prop\n",
    "        for prop in root.findall(\".//mvn:properties/*\", NAMESPACE):\n",
    "            if prop.tag and prop.text:\n",
    "                key = prop.tag.split('}')[-1]\n",
    "                properties[key] = prop.text.strip()\n",
    "\n",
    "        return properties.get(target_prop, \"UNRESOLVED\")\n",
    "    except Exception as e:\n",
    "        return \"UNRESOLVED\"    \n",
    "\n",
    "def load_file_at_commit(repo, commit_hash, file_path):\n",
    "    try:\n",
    "        commit = repo.commit(commit_hash)\n",
    "        blob = commit.tree / file_path\n",
    "        content = blob.data_stream.read().decode('utf-8')\n",
    "        file_cache[file_path] = content\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        print(f\"[DEBUG]Error loading file '{file_path}' at commit '{commit_hash}': {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_commit(repo, commit_hash, changed_files, dependencies_snapshot):\n",
    "    for file_path in changed_files:\n",
    "        if file_path.endswith(\"pom.xml\"):\n",
    "            content = load_file_at_commit(repo, commit_hash, file_path)\n",
    "            if content:\n",
    "                new_dependencies = parse_pom_xml(content, file_path, properties)\n",
    "                dependencies_snapshot[file_path] = new_dependencies\n",
    "    return dependencies_snapshot\n",
    "\n",
    "\n",
    "def process_commits(repo, commits_with_changes):\n",
    "    dependencies_over_time = {}\n",
    "    dependencies_snapshot = {}\n",
    "\n",
    "    for commit_hash, changed_files in tqdm(commits_with_changes.items(), desc=\"Processing commits\"):\n",
    "        dependencies_snapshot = process_commit(repo, commit_hash, changed_files, dependencies_snapshot)\n",
    "        dependencies_over_time[commit_hash] = dependencies_snapshot.copy()\n",
    "\n",
    "    return dependencies_over_time\n",
    "\n",
    "dependencies_over_time = process_commits(repo, sorted_commit_hashes)\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"dependencies_over_time.json\", \"w\") as f:\n",
    "    json.dump(dependencies_over_time, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51ca383-1178-4511-b60a-3432b71a5ac0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## python and javascript parsing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d40fd514-c552-4214-b82a-68fd944de0df",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing commits: 100%|██████████| 276/276 [00:00<00:00, 1319.54it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize cache and data structures\n",
    "file_cache = {}\n",
    "unique_entries = set()\n",
    "dependency_changes = defaultdict(list)\n",
    "\n",
    "# Supported version specifiers in requirements.txt\n",
    "VERSION_SPECIFIERS = [\n",
    "    '==', '>=', '<=', '~=', '!=', '>', '<'\n",
    "]\n",
    "\n",
    "\n",
    "def load_file_at_commit(repo_path, commit_hash, file_path):\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"git\", \"-C\", repo_path, \"show\", f\"{commit_hash}:{file_path}\"],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "        return result.stdout\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error loading file '{file_path}' at commit '{commit_hash}': {e.stderr}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_commit_file_content(repo_path, sha, file_path, type):\n",
    "    content = load_file_at_commit(repo_path, sha, file_path)\n",
    "        \n",
    "    if content:\n",
    "        parse_file_content(content, sha, file_path, type)\n",
    "\n",
    "\n",
    "def parse_file_content(content, sha, filename, type):\n",
    "    dependencies = {}\n",
    "    hash_pattern = re.compile(r\"--hash=sha256:[a-fA-F0-9]{64}\")\n",
    "\n",
    "    if type == \"py\" and \"requirements.txt\" in filename:\n",
    "        for line in content.splitlines():\n",
    "            line = line.strip()\n",
    "\n",
    "            if not line or line.startswith('#') or line.startswith(\"--\"):  # Ignore comments and empty lines\n",
    "                continue\n",
    "\n",
    "            # Check if line is a hash\n",
    "            if hash_pattern.match(line):\n",
    "                continue\n",
    "\n",
    "            # Remove inline comments\n",
    "            if '#' in line:\n",
    "                line = line.split('#', 1)[0].strip()\n",
    "\n",
    "            # Find the version specifier used\n",
    "            version = \"UNSPECIFIED\"\n",
    "            for specifier in VERSION_SPECIFIERS:\n",
    "                if specifier in line:\n",
    "                    package, version = line.split(specifier, 1)\n",
    "                    package = package.strip()\n",
    "                    version = version.strip()\n",
    "                    dependencies[package] = version\n",
    "                    break\n",
    "            else:  # No version specifier found\n",
    "                package = line\n",
    "                dependencies[package] = \"latest-version-available\"\n",
    "\n",
    "    elif type == \"js\" and \"package.json\" in filename:\n",
    "        import json\n",
    "        try:\n",
    "            parsed_json = json.loads(content)\n",
    "            if \"dependencies\" in parsed_json:\n",
    "                dependencies.update(parsed_json[\"dependencies\"])\n",
    "            if \"devDependencies\" in parsed_json:\n",
    "                dependencies.update(parsed_json[\"devDependencies\"])\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON file '{filename}': {e}\")\n",
    "\n",
    "    # Store the parsed dependencies for this commit\n",
    "    dependency_changes[sha] = {\n",
    "        \"filename\": filename,\n",
    "        \"dependencies\": dependencies,\n",
    "        \"ecosystem\": type\n",
    "    }\n",
    "\n",
    "\n",
    "def process_commits(repo_path, commits_with_files):\n",
    "    for sha, files in tqdm(commits_with_files.items(), desc=\"Processing commits\"):\n",
    "        for file_path in files:\n",
    "            if file_path.endswith(\"requirements.txt\"):\n",
    "                process_commit_file_content(repo_path, sha, file_path, \"py\")\n",
    "            elif file_path.endswith(\"package.json\"):\n",
    "                process_commit_file_content(repo_path, sha, file_path, \"js\")\n",
    "\n",
    "\n",
    "def save_results(output_file=\"dependencies_over_time_py.json\"):\n",
    "    import json\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(dependency_changes, f, indent=4)\n",
    "\n",
    "\n",
    "process_commits(repo_path, sorted_commit_hashes)\n",
    "save_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cf913ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing commits: 100%|██████████| 276/276 [00:00<00:00, 560.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'commit': '8960befa37a1e5e04b80cb044aa5121af115dd43', 'file': 'zap/zap.gradle.kts', 'group': 'com.fifesoft', 'artifact': 'rsyntaxtextarea', 'version': '2.5.8'}\n",
      "{'commit': '8960befa37a1e5e04b80cb044aa5121af115dd43', 'file': 'zap/zap.gradle.kts', 'group': 'com.github.zafarkhaja', 'artifact': 'java-semver', 'version': '0.8.0'}\n",
      "{'commit': '8960befa37a1e5e04b80cb044aa5121af115dd43', 'file': 'zap/zap.gradle.kts', 'group': 'com.googlecode.java-diff-utils', 'artifact': 'diffutils', 'version': '1.2.1'}\n",
      "{'commit': '8960befa37a1e5e04b80cb044aa5121af115dd43', 'file': 'zap/zap.gradle.kts', 'group': 'commons-beanutils', 'artifact': 'commons-beanutils', 'version': '1.8.3'}\n",
      "{'commit': '8960befa37a1e5e04b80cb044aa5121af115dd43', 'file': 'zap/zap.gradle.kts', 'group': 'commons-codec', 'artifact': 'commons-codec', 'version': '1.9'}\n",
      "{'commit': '8960befa37a1e5e04b80cb044aa5121af115dd43', 'file': 'zap/zap.gradle.kts', 'group': 'commons-collections', 'artifact': 'commons-collections', 'version': '3.2.2'}\n",
      "{'commit': '8960befa37a1e5e04b80cb044aa5121af115dd43', 'file': 'zap/zap.gradle.kts', 'group': 'commons-configuration', 'artifact': 'commons-configuration', 'version': '1.9'}\n",
      "{'commit': '8960befa37a1e5e04b80cb044aa5121af115dd43', 'file': 'zap/zap.gradle.kts', 'group': 'commons-httpclient', 'artifact': 'commons-httpclient', 'version': '3.1'}\n",
      "{'commit': '8960befa37a1e5e04b80cb044aa5121af115dd43', 'file': 'zap/zap.gradle.kts', 'group': 'commons-io', 'artifact': 'commons-io', 'version': '2.4'}\n",
      "{'commit': '8960befa37a1e5e04b80cb044aa5121af115dd43', 'file': 'zap/zap.gradle.kts', 'group': 'commons-lang', 'artifact': 'commons-lang', 'version': '2.6'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "file_cache = {}\n",
    "\n",
    "def is_gradle_related(file_path):\n",
    "    return (\n",
    "        file_path.endswith(\"build.gradle\")\n",
    "        or file_path.endswith(\"build.gradle.kts\")\n",
    "        or file_path.endswith(\"settings.gradle\")\n",
    "        or file_path.endswith(\"settings.gradle.kts\")\n",
    "        or file_path.endswith(\"gradle.properties\")\n",
    "        or file_path.endswith(\".gradle\")\n",
    "        or file_path.endswith(\".gradle.kts\")\n",
    "    )\n",
    "\n",
    "def parse_gradle_properties(content):\n",
    "    props = {}\n",
    "    for line in content.splitlines():\n",
    "        line = line.strip()\n",
    "        if line and not line.startswith(\"#\") and \"=\" in line:\n",
    "            key, value = line.split(\"=\", 1)\n",
    "            props[key.strip()] = value.strip()\n",
    "    return props\n",
    "\n",
    "\n",
    "def extract_dependencies_block(content: str) -> str:\n",
    "    \"\"\"Returns only the content inside dependencies { ... } block(s).\"\"\"\n",
    "    lines = content.splitlines()\n",
    "    inside = False\n",
    "    depth = 0\n",
    "    collected = []\n",
    "\n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "        if stripped.startswith(\"dependencies\"):\n",
    "            if \"{\" in stripped:\n",
    "                inside = True\n",
    "                depth = stripped.count(\"{\") - stripped.count(\"}\")\n",
    "                continue\n",
    "\n",
    "        if inside:\n",
    "            if \"{\" in stripped:\n",
    "                depth += stripped.count(\"{\")\n",
    "            if \"}\" in stripped:\n",
    "                depth -= stripped.count(\"}\")\n",
    "            collected.append(line)\n",
    "            if depth <= 0:\n",
    "                inside = False\n",
    "\n",
    "    return \"\\n\".join(collected)\n",
    "\n",
    "\n",
    "def parse_gradle_dependencies(file_content: str, properties: Dict[str, str], commit_sha: str, file_path: str) -> List[Dict[str, Any]]:\n",
    "    dependencies = []\n",
    "\n",
    "    # NEW: extract only content inside dependencies block(s)\n",
    "    block = extract_dependencies_block(file_content)\n",
    "\n",
    "    # Patterns for Kotlin + Groovy\n",
    "    patterns = [\n",
    "        re.compile(r'(?P<config>\\w+)\\s*\\(\\s*[\"\\'](?P<group>[^:\\'\"]+):(?P<artifact>[^:\\'\"]+):(?P<version>[^\\'\"]+)[\"\\']\\s*\\)'),  # Kotlin DSL\n",
    "        re.compile(r'(?P<config>\\w+)\\s+[\"\\'](?P<group>[^:\\'\"]+):(?P<artifact>[^:\\'\"]+):(?P<version>[^\\'\"]+)[\"\\']')  # Groovy DSL\n",
    "    ]\n",
    "\n",
    "    for pattern in patterns:\n",
    "        for match in pattern.finditer(block):\n",
    "            config = match.group(\"config\")\n",
    "            group = match.group(\"group\")\n",
    "            artifact = match.group(\"artifact\")\n",
    "            version = match.group(\"version\")\n",
    "\n",
    "            version_source = \"literal\"\n",
    "            if version.startswith(\"$\") or \"${\" in version:\n",
    "                var_name = version.replace(\"$\", \"\").replace(\"{\", \"\").replace(\"}\", \"\")\n",
    "                if var_name in properties:\n",
    "                    version = properties[var_name]\n",
    "                    version_source = \"resolved_from_property\"\n",
    "                else:\n",
    "                    version_source = \"unresolved\"\n",
    "\n",
    "            if \"$\" not in group and \"$\" not in artifact and \"$\" not in version:\n",
    "                dependencies.append({\n",
    "                    \"commit\": commit_sha,\n",
    "                    \"file\": file_path,\n",
    "                    #\"tool\": \"gradle\",\n",
    "                    #\"configuration\": config,\n",
    "                    \"group\": group,\n",
    "                    \"artifact\": artifact,\n",
    "                    \"version\": version,\n",
    "                    #\"version_source\": version_source\n",
    "                })\n",
    "\n",
    "    return dependencies\n",
    "\n",
    "\n",
    "def load_file_at_commit(repo, commit_hash, file_path):\n",
    "    try:\n",
    "        commit = repo.commit(commit_hash)\n",
    "        blob = commit.tree / file_path\n",
    "        content = blob.data_stream.read().decode('utf-8')\n",
    "        file_cache[file_path] = content\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        print(f\"[DEBUG] Error loading file '{file_path}' at commit '{commit_hash}': {e}\")\n",
    "        return None\n",
    "    \n",
    "def parse_local_kotlin_variables(content: str) -> Dict[str, str]:\n",
    "    props = {}\n",
    "    # Matches lines like: val log4jVersion = \"2.20.0\" or var version = \"1.0\"\n",
    "    pattern = re.compile(r'(val|var)\\s+(\\w+)\\s*=\\s*[\"\\']([^\"\\']+)[\"\\']')\n",
    "    for match in pattern.finditer(content):\n",
    "        var_name = match.group(2)\n",
    "        value = match.group(3)\n",
    "        props[var_name] = value\n",
    "    return props\n",
    "\n",
    "def process_commit(repo, commit_hash, changed_files, dependencies_snapshot, all_dependencies, properties_by_commit):\n",
    "    for file_path in changed_files:\n",
    "        if not is_gradle_related(file_path):\n",
    "            continue\n",
    "\n",
    "        content = load_file_at_commit(repo, commit_hash, file_path)\n",
    "        if content:\n",
    "            dependencies_snapshot[file_path] = content\n",
    "\n",
    "            # Parse gradle.properties\n",
    "            if file_path.endswith(\"gradle.properties\"):\n",
    "                props = parse_gradle_properties(content)\n",
    "                properties_by_commit[commit_hash] = props\n",
    "\n",
    "            # Parse dependencies from .gradle or .gradle.kts files\n",
    "            elif file_path.endswith(\".gradle\") or file_path.endswith(\".gradle.kts\"):\n",
    "                props = {}\n",
    "                #props.update(parse_ext_block(content))                  # ext { ... }\n",
    "                #props.update(parse_extra_assignments(content))          # extra[\"...\"] = ...\n",
    "                #props.update(extract_project_properties(content))       # group/version/name\n",
    "                props.update(parse_local_kotlin_variables(content))     # val/var = \"...\"\n",
    "\n",
    "                if commit_hash not in properties_by_commit:\n",
    "                    properties_by_commit[commit_hash] = {}\n",
    "                properties_by_commit[commit_hash].update(props)\n",
    "\n",
    "                resolved_props = properties_by_commit[commit_hash]\n",
    "                parsed_deps = parse_gradle_dependencies(content, resolved_props, commit_hash, file_path)\n",
    "                all_dependencies.extend(parsed_deps)\n",
    "\n",
    "    return dependencies_snapshot, all_dependencies\n",
    "\n",
    "def process_commits(repo, commits_with_changes):\n",
    "    dependencies_over_time = {}\n",
    "    dependencies_snapshot = {}\n",
    "    all_dependencies = []\n",
    "    properties_by_commit = {}\n",
    "\n",
    "    for commit_hash, changed_files in tqdm(commits_with_changes.items(), desc=\"Processing commits\"):\n",
    "        dependencies_snapshot, all_dependencies = process_commit(\n",
    "            repo, commit_hash, changed_files, dependencies_snapshot, all_dependencies, properties_by_commit\n",
    "        )\n",
    "        dependencies_over_time[commit_hash] = dependencies_snapshot.copy()\n",
    "\n",
    "    return dependencies_over_time, all_dependencies\n",
    "\n",
    "\n",
    "dependencies_over_time, all_dependencies = process_commits(repo, sorted_commit_hashes)\n",
    "\n",
    "# Optionally print the parsed dependencies\n",
    "#for dep in all_dependencies:\n",
    "    #print(dep)\n",
    "\n",
    "cleaned_dependencies = [\n",
    "    dep for dep in all_dependencies\n",
    "    if \"$\" not in dep[\"group\"]\n",
    "    and \"$\" not in dep[\"artifact\"]\n",
    "    and \"$\" not in dep[\"version\"]\n",
    "]\n",
    "\n",
    "for dep in cleaned_dependencies [:10]:\n",
    "    print(dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7032ee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_gradle_dependencies(dependencies):\n",
    "    grouped = defaultdict(lambda: {\"gradle\": defaultdict(dict)})\n",
    "\n",
    "    for dep in dependencies:\n",
    "        commit = dep[\"commit\"]\n",
    "        file = dep[\"file\"]\n",
    "        group_artifact = f\"{dep['group']}:{dep['artifact']}\"\n",
    "        version = dep[\"version\"]\n",
    "\n",
    "        grouped[commit][\"gradle\"][file][group_artifact] = version\n",
    "\n",
    "    return grouped\n",
    "\n",
    "grouped_dependencies = group_gradle_dependencies(cleaned_dependencies)\n",
    "import json\n",
    "# Convert the defaultdict to a normal dict recursively\n",
    "def to_dict(d):\n",
    "    if isinstance(d, defaultdict):\n",
    "        d = {k: to_dict(v) for k, v in d.items()}\n",
    "    elif isinstance(d, dict):\n",
    "        d = {k: to_dict(v) for k, v in d.items()}\n",
    "    return d\n",
    "\n",
    "#print(json.dumps(to_dict(grouped_dependencies), indent=2))\n",
    "\n",
    "def save_results(data, output_file=\"dependencies_over_time.json\"):\n",
    "    import json\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "save_results(to_dict(grouped_dependencies), \"dependencies_over_time_gradle.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30441fca-0050-4127-850a-3e1c6c10b71d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_dependencies(python_and_javascript: Dict, java: Dict, gradle: Dict) -> Dict:\n",
    "    merged_data = {}\n",
    "\n",
    "    # Combine all unique commit hashes from both datasets\n",
    "    all_commits = set(python_and_javascript) | set(java) | set(gradle)\n",
    "\n",
    "    for commit in all_commits:\n",
    "        merged_data[commit] = {\n",
    "            \"java\": {},\n",
    "            \"gradle\": {},\n",
    "            \"python\": {},\n",
    "            \"javascript\": {}\n",
    "        }\n",
    "\n",
    "        # Add Java dependencies if present\n",
    "        if commit in java:\n",
    "            for file, content in java[commit].items():\n",
    "                merged_data[commit][\"java\"][file] = content  # No renaming!\n",
    "        \n",
    "        if commit in gradle:\n",
    "            gradle_files = gradle[commit].get(\"gradle\", {})  # Extract only the inner part\n",
    "            for file, content in gradle_files.items():\n",
    "                merged_data[commit][\"gradle\"][file] = content\n",
    "\n",
    "        # Add Python and JavaScript dependencies if present\n",
    "        if commit in python_and_javascript:\n",
    "            content = python_and_javascript[commit]\n",
    "\n",
    "            # Check if it's a Python or JavaScript file based on 'ecosystem'\n",
    "            if content.get(\"ecosystem\") == \"py\":\n",
    "                merged_data[commit][\"python\"][content[\"filename\"]] = content[\"dependencies\"]\n",
    "            elif content.get(\"ecosystem\") == \"js\":\n",
    "                merged_data[commit][\"javascript\"][content[\"filename\"]] = content[\"dependencies\"]\n",
    "\n",
    "\n",
    "    return merged_data\n",
    "\n",
    "def remove_empty_objects(data):\n",
    "    if isinstance(data, dict):\n",
    "        return {\n",
    "            k: remove_empty_objects(v)\n",
    "            for k, v in data.items()\n",
    "            if not (isinstance(v, dict) and not v)  # remove empty dicts\n",
    "        }\n",
    "    elif isinstance(data, list):\n",
    "        return [remove_empty_objects(item) for item in data]\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "def save_merged_data(merged_data: Dict, output_file: str):\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(merged_data, f, indent=4)\n",
    "\n",
    "\n",
    "with open('dependencies_over_time_py.json', 'r') as f:\n",
    "    python_and_javascript = json.load(f)\n",
    "\n",
    "#with open('dependencies_over_time.json', 'r') as f:\n",
    "#    java = json.load(f)\n",
    "\n",
    "with open('dependencies_over_time_gradle.json', 'r') as f:\n",
    "    gradle = json.load(f)\n",
    "\n",
    "merged_data = merge_dependencies(python_and_javascript, {}, gradle)\n",
    "new_merged_data = remove_empty_objects(merged_data)\n",
    "save_merged_data(new_merged_data, 'merged_dependencies.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52bddaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "\n",
    "# Load input data\n",
    "with open(\"merged_dependencies.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Load commit-date mapping\n",
    "# This holds the last-known state per (system, filename)\n",
    "file_state = {}\n",
    "\n",
    "commit_dates = commits_with_date\n",
    "\n",
    "# Final result\n",
    "changes = []\n",
    "\n",
    "for commit, commit_data in data.items():\n",
    "    date = commit_dates.get(commit)\n",
    "\n",
    "    for system, files in commit_data.items():\n",
    "        for filename, curr_deps in files.items():\n",
    "            file_key = (system, filename)\n",
    "            prev_deps = file_state.get(file_key, {})\n",
    "\n",
    "            # Compare current and previous\n",
    "            '''for dep, old_version in prev_deps.items():\n",
    "                if dep not in curr_deps:\n",
    "                    # Removed\n",
    "                    changes.append({\n",
    "                        \"commit\": commit,\n",
    "                        \"date\": date,\n",
    "                        \"filename\": filename,\n",
    "                        \"dependency\": dep,\n",
    "                        \"change_type\": \"removed\",\n",
    "                        \"old_version\": old_version,\n",
    "                        \"new_version\": None\n",
    "                    })\n",
    "                elif curr_deps[dep] != old_version:\n",
    "                    # Updated\n",
    "                    changes.append({\n",
    "                        \"commit\": commit,\n",
    "                        \"date\": date,\n",
    "                        \"filename\": filename,\n",
    "                        \"dependency\": dep,\n",
    "                        \"change_type\": \"updated\",\n",
    "                        \"old_version\": old_version,\n",
    "                        \"new_version\": curr_deps[dep]\n",
    "                    })'''\n",
    "\n",
    "            for dep, new_version in curr_deps.items():\n",
    "                if dep not in prev_deps:\n",
    "                    # Added\n",
    "                    changes.append({\n",
    "                        \"commit\": commit,\n",
    "                        \"date\": date,\n",
    "                        \"filename\": filename,\n",
    "                        \"dependency\": dep,\n",
    "                        #\"change_type\": \"added\",\n",
    "                        #\"old_version\": None,\n",
    "                        \"new_version\": new_version\n",
    "                    })\n",
    "\n",
    "            # IMPORTANT: store a deep copy to avoid shared references\n",
    "            file_state[file_key] = copy.deepcopy(curr_deps)\n",
    "\n",
    "# Save the changes\n",
    "with open(\"dependency_changes.json\", \"w\") as f:\n",
    "    json.dump(changes, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e07be4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bak",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
